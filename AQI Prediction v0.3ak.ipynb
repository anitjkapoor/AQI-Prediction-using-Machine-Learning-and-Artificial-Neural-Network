{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 AQI Prediction - Beijing\n",
    "\n",
    "### As a part of this project, we will try to scrape climate data and merge it with PM2.5 AQI of beijing and try to predict the AQI given climatic conditions of Beijing.\n",
    "\n",
    "#### We will be using different web scraping techniques, machine learning algorithms such as Decision Tress, Linear Regression and Rnadom Forest and compare their performance with different architectures of Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from the website for all days between Jan 2010 and Dec 2014\n",
    "base_url = 'https://en.tutiempo.net/climate/' # URL of the website\n",
    "city ='ws-545110.html' # City Code:: Beijing \n",
    "dates = []\n",
    "for i in range(2010,2015):\n",
    "    for j in range(1,13):\n",
    "        month = str(j).rjust(2,'0')\n",
    "        date = month+ '-' + str(i)\n",
    "        dates.append(date)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url looks something like this -------> https://en.tutiempo.net/climate/01-2000/ws-430630.html\n",
    "# Get all webpages and store them in a list\n",
    "\n",
    "webpages = []\n",
    "\n",
    "for i,date in enumerate(dates):\n",
    "    url = base_url+date+ '/' + city\n",
    "    temp = requests.get(url)\n",
    "    temp_tex = temp.text\n",
    "    webpages.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bs4 to extract relevant information\n",
    "\n",
    "for page,date in zip(webpages,dates):\n",
    "    soup = BeautifulSoup(page.text.encode('utf-8'),'lxml')\n",
    "    table = soup.findAll('table',{'class': 'medias mensuales numspan'})\n",
    "    extract = []\n",
    "    for body in table:\n",
    "        for row in body:\n",
    "            temp = []\n",
    "            for element in (row):\n",
    "                a =  element.get_text()\n",
    "                temp.append(a)\n",
    "            extract.append(temp)\n",
    "        extract = pd.DataFrame(extract)\n",
    "        extract.columns = extract.iloc[0,:]\n",
    "        extract = extract.iloc[1:extract.shape[0]-2,:]\n",
    "        \n",
    "    ind = list(extract['Day'])\n",
    "    for i in range(len(ind)):\n",
    "        ind[i] = str(ind[i]).rjust(2,'0') + '-' + date\n",
    "    extract.index = ind\n",
    "    if date == '01-2010':\n",
    "        climate_df = extract.copy()\n",
    "    else:\n",
    "        climate_df = climate_df.append(extract)\n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>SLP</th>\n",
       "      <th>H</th>\n",
       "      <th>PP</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>VG</th>\n",
       "      <th>RA</th>\n",
       "      <th>SN</th>\n",
       "      <th>TS</th>\n",
       "      <th>FG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01-01-2010</th>\n",
       "      <td>1</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>-</td>\n",
       "      <td>45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>o</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02-01-2010</th>\n",
       "      <td>2</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-9</td>\n",
       "      <td>-</td>\n",
       "      <td>83</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>o</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03-01-2010</th>\n",
       "      <td>3</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-13</td>\n",
       "      <td>-</td>\n",
       "      <td>76</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>39.6</td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>o</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04-01-2010</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05-01-2010</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0          Day     T    TM     Tm SLP   H    PP   VV     V    VM VG RA SN TS  \\\n",
       "01-01-2010   1  -4.7    -1  -10.2   -  45  0.51    8   7.6  14.4  -     o      \n",
       "02-01-2010   2  -5.7  -3.8     -9   -  83   3.3  2.7  10.6    18  -     o      \n",
       "03-01-2010   3  -9.6  -6.4    -13   -  76   6.6    5  17.8  39.6  -     o      \n",
       "04-01-2010   4                                                                 \n",
       "05-01-2010   5                                                                 \n",
       "\n",
       "0          FG  \n",
       "01-01-2010     \n",
       "02-01-2010     \n",
       "03-01-2010     \n",
       "04-01-2010     \n",
       "05-01-2010     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31']\n",
      "................................................................................\n",
      "T\n",
      "['-4.7' '-5.7' '-9.6' '' '-12.4' '-10.9' '-8.7' '-11.4' '-6.9' '-7.9'\n",
      " '0.7' '-3.4' '-5.5' '-4.4' '-3.5' '1.9' '0.3' '-4.6' '-5.9' '-4.9' '-4'\n",
      " '-1.2' '0.8' '-2.9' '-5.2' '-6.2' '0.6' '1.8' '4.4' '3.5' '1.1' '-1.1'\n",
      " '-2.7' '2.5' '1.7' '-2' '-4.1' '-3.2' '3.6' '0.4' '0.2' '5.3' '5.7' '8.6'\n",
      " '4.1' '8.3' '10.8' '7.6' '5.5' '10.7' '14.2' '15.7' '8.9' '12.8' '14.7'\n",
      " '10.9' '11.8' '8.2' '9.2' '23.4' '23' '20.1' '20.6' '19.9' '16.2' '19.4'\n",
      " '18.3' '22.6' '25' '26.7' '21.9' '26.2' '25.7' '19.5' '21.6' '21.2'\n",
      " '25.9' '25.5' '22.3' '25.2' '27.7' '27.1' '26' '23.1' '29.3' '31.6'\n",
      " '27.2' '30.1' '24.3' '24.7' '23.9' '24.1' '30.3' '30.4' '27.9' '26.5'\n",
      " '26.3' '24.4' '26.6' '27.3' '29.5' '27.5' '25.1' '26.4' '23.6' '25.4'\n",
      " '24.5' '23.7' '23.3' '24.6' '25.3' '25.6' '19.7' '16.8' '13.4' '16.9'\n",
      " '17.6' '18.9' '16.7' '16.4' '16.6' '15.6' '15.2' '15.4' '9.1' '9.4' '11'\n",
      " '7.2' '2.7' '10.6' '6.3' '9.8' '3.2' '2.6' '4.9' '4' '-0.8' '0.9' '-1.4'\n",
      " '2.9' '1.6' '-7.6' '-6.3' '3.8' '-0.9' '-7.7' '-4.2' '-7.2' '-6.8' '-3.9'\n",
      " '-5.3' '-2.6' '-8.1' '-6.6' '-6.4' '-5.6' '1' '-0.3' '-1.8' '-0.1' '-1.7'\n",
      " '-5.8' '-5' '3.3' '6.2' '6.1' '10.2' '5.9' '4.5' '8.8' '5.4' '4.6' '10.3'\n",
      " '7.1' '11.9' '18.2' '17.8' '16' '17.9' '12.2' '20.8' '20' '19.8' '20.3'\n",
      " '14.9' '20.2' '23.8' '19' '20.9' '19.3' '22.9' '23.5' '28.4' '26.9'\n",
      " '28.5' '24.8' '28.9' '24.9' '28.7' '27.4' '26.1' '28.2' '27' '29.2'\n",
      " '27.6' '22.7' '22.1' '22' '22.5' '20.4' '21.8' '15.9' '17.7' '19.2'\n",
      " '13.6' '17.1' '18.8' '16.3' '13.7' '12.9' '8' '8.7' '12' '12.5' '5.8'\n",
      " '0.5' '3.1' '-0.2' '-0.7' '-0.5' '-0.4' '-2.1' '-3.8' '-3' '-3.1' '-5.1'\n",
      " '-2.3' '-9.2' '-8.2' '-4.3' '-6.7' '-1.6' '-2.5' '2.2' '7.3' '2.4' '6.4'\n",
      " '9.9' '7.8' '8.1' '14.3' '14.4' '12.7' '13.3' '18.7' '17.2' '21.4' '24.2'\n",
      " '22.4' '19.1' '26.8' '25.8' '28.8' '27.8' '24' '21.1' '17.3' '17.4'\n",
      " '12.4' '13.8' '14.6' '13.9' '3' '-4.8' '-7' '-10.7' '-11.1' '-8.4' '-6.5'\n",
      " '-10.5' '-10.8' '-7.1' '-2.8' '-3.6' '-2.4' '-1.3' '-3.3' '-9' '-7.4'\n",
      " '-1.5' '7.7' '-0.6' '6.8' '8.5' '3.4' '6' '1.2' '2.1' '10.4' '8.4' '11.5'\n",
      " '5.6' '18' '23.2' '15.5' '21.5' '19.6' '22.8' '22.2' '29.8' '30.2' '29.6'\n",
      " '29.1' '29.4' '21.7' '16.1' '9' '9.7' '13' '7.9' '4.7' '7.5' '3.7' '1.4'\n",
      " '-2.2' '6.6' '4.8' '-1.9' '1.3' '10.1' '12.6' '14.1' '21' '11.2' '18.5'\n",
      " '18.1' '20.7' '28.3' '32.7' '29.9' '31.9' '32.2' '28.1' '29.7' '11.4'\n",
      " '15' '14.8' '4.3' '-']\n",
      "................................................................................\n",
      "TM\n",
      "['-1' '-3.8' '-6.4' '' '-4.4' '-7' '-2.8' '-5' '-1.6' '-0.5' '4.8' '5.5'\n",
      " '-1.2' '2' '8.8' '6.8' '1' '-1.5' '5.2' '4.4' '3.5' '1.9' '11' '9.6'\n",
      " '14.3' '8' '5' '0.8' '-0.2' '9.1' '6' '-1.4' '1.3' '8.5' '3' '9' '10'\n",
      " '15.1' '17' '12.7' '12' '16' '20.3' '21.8' '16.8' '11.2' '13.6' '18'\n",
      " '19.2' '13.4' '21' '13' '14.6' '32.2' '32.6' '24.6' '28.6' '25.5' '22.2'\n",
      " '25.6' '23.3' '29' '34' '35' '29.1' '31.9' '31' '27.8' '26.8' '25' '28'\n",
      " '32' '33' '30.2' '33.6' '31.6' '25.4' '37' '38' '30.1' '30.3' '26.4'\n",
      " '33.3' '35.7' '34.9' '36' '33.7' '33.2' '32.4' '30.4' '30' '35.4' '23.8'\n",
      " '29.3' '29.8' '26.6' '27.7' '27' '24' '22' '22.7' '22.4' '23.7' '24.3'\n",
      " '23' '21.1' '20.7' '11.5' '14' '8.3' '11.3' '15.8' '13.9' '19.5' '14.7'\n",
      " '11.8' '10.5' '6.4' '13.8' '11.4' '7.1' '8.1' '5.9' '-1.3' '-2.4' '7.4'\n",
      " '-2.1' '6.9' '-2.9' '0.1' '0' '-1.9' '4' '-2.7' '9.3' '7' '4.6' '8.2'\n",
      " '-2' '2.7' '1.2' '7.5' '9.8' '11.1' '12.6' '20' '9.2' '10.3' '15' '18.8'\n",
      " '19' '26' '24.1' '23.2' '17.3' '18.2' '21.2' '27.3' '21.4' '26.9' '27.4'\n",
      " '20.8' '26.1' '34.8' '33.1' '35.5' '31.2' '28.2' '31.3' '32.1' '29.6'\n",
      " '35.9' '34.2' '27.1' '27.2' '29.4' '22.3' '25.8' '22.5' '20.4' '18.1'\n",
      " '17.2' '15.5' '17.5' '16.4' '16.2' '10.6' '11.6' '4.1' '0.2' '5.1' '1.6'\n",
      " '3.8' '3.6' '2.3' '3.2' '2.4' '2.1' '2.2' '1.7' '-0.8' '2.6' '3.9' '0.3'\n",
      " '-0.9' '7.7' '8.6' '4.3' '6.7' '5.3' '2.5' '12.8' '10.4' '15.2' '18.9'\n",
      " '12.2' '16.3' '16.1' '16.5' '23.6' '25.3' '30.7' '24.4' '31.8' '29.2'\n",
      " '35.3' '36.2' '36.5' '32.3' '32.8' '28.9' '30.5' '29.9' '21.6' '33.4'\n",
      " '31.1' '23.9' '27.9' '25.9' '24.7' '19.4' '12.4' '20.2' '13.1' '7.2'\n",
      " '8.7' '9.9' '-0.6' '1.4' '3.3' '1.8' '-0.4' '-2.5' '-4.3' '-2.3' '-6.3'\n",
      " '-3.7' '-0.7' '0.7' '3.1' '-2.6' '6.2' '19.6' '8.4' '6.1' '13.7' '18.5'\n",
      " '16.9' '13.2' '21.5' '20.6' '36.8' '26.2' '31.4' '27.5' '32.9' '23.1'\n",
      " '15.4' '14.8' '7.6' '5.4' '1.5' '2.8' '7.8' '12.3' '-1.7' '0.6' '9.5'\n",
      " '5.6' '22.1' '28.8' '32.5' '31.7' '32.7' '29.5' '25.1' '4.2' '0.5' '-']\n",
      "................................................................................\n",
      "Tm\n",
      "['-10.2' '-9' '-13' '' '-17' '-16' '-15' '-14' '-2' '-10' '-3' '-5' '-12'\n",
      " '-6.1' '-9.5' '-8' '-4' '-0.2' '-6' '-4.8' '-1.9' '-1' '-7' '3' '0' '2.4'\n",
      " '1' '5' '2' '5.1' '9.3' '6' '8' '2.8' '10.4' '13' '12' '15' '12.6' '15.4'\n",
      " '14' '20' '21' '17' '16.8' '19' '18.6' '21.2' '18.4' '20.4' '20.9' '20.7'\n",
      " '20.5' '24.5' '22' '23' '22.5' '24' '21.7' '21.6' '23.1' '26' '21.1'\n",
      " '21.3' '18' '20.3' '16' '22.1' '15.6' '7' '10' '13.4' '11' '6.5' '7.4'\n",
      " '1.6' '-0.1' '-4.5' '-11' '-9.2' '-11.4' '-9.4' '-6.3' '-4.1' '-3.8' '4'\n",
      " '2.1' '-1.1' '-0.3' '4.3' '1.8' '3.7' '8.4' '11.2' '7.5' '9' '11.9'\n",
      " '16.7' '14.6' '19.6' '21.9' '25' '19.7' '19.9' '23.4' '24.3' '23.8'\n",
      " '22.9' '19.2' '15.2' '5.3' '9.5' '12.2' '10.5' '10.2' '7.8' '5.2' '-1.8'\n",
      " '-3.4' '-4.4' '-9.8' '-3.3' '-2.9' '-0.8' '0.8' '3.5' '2.5' '11.6' '9.7'\n",
      " '11.3' '7.6' '10.8' '17.8' '13.9' '16.1' '22.6' '24.6' '19.8' '17.3'\n",
      " '18.1' '18.2' '16.6' '17.7' '6.2' '7.9' '1.1' '0.6' '-3.2' '-1.4' '-2.1'\n",
      " '-7.4' '-7.2' '-11.2' '-8.3' '-4.9' '-0.6' '-0.9' '1.4' '5.4' '1.9' '2.7'\n",
      " '4.9' '8.2' '16.4' '11.7' '18.5' '19.4' '18.3' '18.7' '19.5' '21.4'\n",
      " '22.7' '25.8' '24.2' '22.8' '17.9' '9.8' '-3.5' '-3.9' '-9.1' '-5.4'\n",
      " '0.7' '-2.7' '9.6' '9.9' '12.5' '10.7' '14.3' '15.8' '22.3' '20.2' '23.9'\n",
      " '21.5' '23.5' '26.4' '23.3' '25.7' '17.4' '15.9' '17.2' '8.8' '12.1'\n",
      " '9.1' '-1.7' '-5.6' '-']\n",
      "................................................................................\n",
      "SLP\n",
      "['-' '' '1018.8']\n",
      "................................................................................\n",
      "H\n",
      "['45' '83' '76' '' '52' '64' '55' '47' '43' '72' '34' '29' '28' '33' '61'\n",
      " '36' '75' '31' '25' '30' '50' '78' '82' '44' '60' '62' '59' '35' '80'\n",
      " '27' '24' '39' '32' '18' '20' '40' '63' '58' '26' '23' '19' '49' '41'\n",
      " '77' '68' '71' '56' '67' '53' '84' '48' '54' '38' '73' '81' '89' '70'\n",
      " '57' '85' '42' '87' '79' '69' '46' '86' '37' '22' '66' '21' '16' '12'\n",
      " '17' '14' '65' '51' '90' '74' '91' '88' '93' '95' '92' '97' '13' '11' '7'\n",
      " '8' '-']\n",
      "................................................................................\n",
      "PP\n",
      "['0.51' '3.3' '6.6' '' '0' '-' '2.03' '0.76' '8.13' '7.62' '1.02' '10.41'\n",
      " '5.33' '48.51' '39.88' '10.67' '3.05' '2.79' '2.54' '17.53' '10.16'\n",
      " '77.72' '23.88' '21.08' '5.08' '0.25' '3.56' '6.35' '17.27' '19.05'\n",
      " '2.29' '1.27' '20.83' '14.73' '29.72' '4.32' '1.78' '10.92' '43.69'\n",
      " '30.48' '11.43' '69.6' '42.42' '23.62' '16.51' '26.92' '18.8' '21.59'\n",
      " '18.29' '1.52' '28.45' '160.27' '33.78' '14.99' '13.97' '42.93' '4.06'\n",
      " '12.45' '3.81' '4.57' '46.48' '40.13' '44.7' '16' '12.7' '25.91' '9.65'\n",
      " '8.89' '5.84' '20.32' '47.75' '12.19' '6.86' '85.09' '15.49']\n",
      "................................................................................\n",
      "VV\n",
      "['8' '2.7' '5' '' '9' '4.8' '9.7' '10.9' '9.2' '2.6' '10.3' '10.8' '9.8'\n",
      " '6.8' '9.3' '10' '10.1' '4.2' '3.9' '8.9' '8.5' '10.5' '5.3' '7.1' '6.1'\n",
      " '8.7' '3.1' '2.1' '8.2' '7.2' '11.4' '9.5' '7.9' '2.4' '6.4' '7.7' '6.6'\n",
      " '5.1' '5.6' '8.4' '4' '6' '5.5' '6.9' '1.4' '6.3' '3.5' '3.7' '3.2' '7.4'\n",
      " '10.6' '7.6' '1.9' '2.3' '1.6' '11.6' '2.9' '4.7' '5.8' '3.4' '11.3'\n",
      " '1.8' '4.3' '12.4' '4.5' '11.1' '1.3' '-' '15' '13.2' '12.1' '12.7'\n",
      " '11.7' '14' '1.1']\n",
      "................................................................................\n",
      "V\n",
      "['7.6' '10.6' '17.8' '' '6.9' '5.7' '11.5' '13.5' '13.9' '7' '9.6' '28.2'\n",
      " '17' '14.1' '6.5' '18.1' '10.2' '8.7' '7.4' '12.4' '4.8' '5.9' '11.7'\n",
      " '7.8' '6.7' '12.2' '9.1' '14.4' '15.2' '16.3' '11.9' '19.3' '8.5' '24.8'\n",
      " '10.9' '12' '13' '17.6' '20' '19.6' '13.1' '17.2' '13.7' '14.6' '21.3'\n",
      " '23.3' '13.3' '16.1' '14.8' '19.1' '15.7' '10' '8.3' '12.8' '9.4' '5.6'\n",
      " '6.3' '6.1' '8.1' '10.4' '9.3' '5.4' '8' '9.8' '20.9' '4.4' '3.9' '3.5'\n",
      " '22.6' '25.6' '5.2' '17.4' '19.8' '18.7' '23.7' '14.3' '23.5' '23.2' '15'\n",
      " '7.2' '26.3' '34.8' '28' '16.7' '16.9' '15.9' '10.7' '8.9' '23' '15.4'\n",
      " '25.9' '30' '25.4' '19.4' '15.6' '11.1' '28.5' '20.6' '21.9' '11.3'\n",
      " '12.6' '4.6' '3.7' '4.1' '21.1' '5' '27.4' '29.4' '22.8' '28.7' '20.7'\n",
      " '30.7' '24.3' '22.4' '31.7' '26.9' '32.2' '18.5' '16.5' '18.3' '30.2'\n",
      " '4.3' '27.8' '26.5' '21.7' '29.3' '31.1' '29.8' '-']\n",
      "................................................................................\n",
      "VM\n",
      "['14.4' '18' '39.6' '' '21.7' '25.2' '32.4' '46.9' '35.9' '10.7' '28.7'\n",
      " '53.9' '43.2' '50.4' '7.2' '57.6' '61.1' '64.8' '-']\n",
      "................................................................................\n",
      "VG\n",
      "['-' '' '46.9' '64.8' '53.9' '43.2' '57.6' '28.7' '35.9' '50.4' '68.3'\n",
      " '75.6' '39.6' '32.4' '61.1' '25.2' '79.1' '82.8' '72' '21.7' '97' '90']\n",
      "................................................................................\n",
      "RA\n",
      "['\\xa0' 'o' '-']\n",
      "................................................................................\n",
      "SN\n",
      "['o' '\\xa0' '-']\n",
      "................................................................................\n",
      "TS\n",
      "['\\xa0' 'o' '-']\n",
      "................................................................................\n",
      "FG\n",
      "['\\xa0' 'o' '-']\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "for i in climate_df.columns:\n",
    "    print(i)\n",
    "    print(climate_df[i].unique())\n",
    "    print('................................................................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>H</th>\n",
       "      <th>PP</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01-01-2010</th>\n",
       "      <td>-4.7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02-01-2010</th>\n",
       "      <td>-5.7</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-9</td>\n",
       "      <td>83</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03-01-2010</th>\n",
       "      <td>-9.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-13</td>\n",
       "      <td>76</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>39.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04-01-2010</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05-01-2010</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0              T    TM     Tm   H    PP   VV     V    VM\n",
       "01-01-2010  -4.7    -1  -10.2  45  0.51    8   7.6  14.4\n",
       "02-01-2010  -5.7  -3.8     -9  83   3.3  2.7  10.6    18\n",
       "03-01-2010  -9.6  -6.4    -13  76   6.6    5  17.8  39.6\n",
       "04-01-2010                                              \n",
       "05-01-2010                                              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove VG,FG,TS,SN,RA, SLP\n",
    "emp_cols = ['VG','FG','TS','SN','RA','SLP','Day']\n",
    "climate_df2 = climate_df.drop(emp_cols,axis=1)\n",
    "climate_df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing AQI Data\n",
    "# The Data for AQI has been downloaded from UCI Machine Learning Repository\n",
    "## Link ------> https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data\n",
    "os.chdir('C:/Users/Aditya Kapoor/Desktop/Data Science Upskilling/Project 3 -- AQI Prediction/Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "[2010, 2011, 2012, 2013, 2014]\n",
      ".........................................................................................\n",
      "month\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      ".........................................................................................\n",
      "day\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      ".........................................................................................\n",
      "hour\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      ".........................................................................................\n",
      "pm2.5\n",
      "[nan, 129.0, 148.0, 159.0, 181.0, 138.0, 109.0, 105.0, 124.0, 120.0, 132.0, 140.0, 152.0, 164.0, 158.0, 154.0, 170.0, 149.0, 156.0, 126.0, 90.0, 63.0, 65.0, 55.0, 83.0, 91.0, 86.0, 82.0, 78.0, 98.0, 107.0, 96.0, 95.0, 70.0, 61.0, 53.0, 71.0, 72.0, 76.0, 73.0, 79.0, 58.0, 25.0, 26.0, 28.0, 20.0, 29.0, 27.0, 32.0, 30.0, 31.0, 33.0, 34.0, 36.0, 39.0, 41.0, 50.0, 56.0, 59.0, 60.0, 84.0, 106.0, 66.0, 77.0, 44.0, 21.0, 42.0, 48.0, 49.0, 52.0, 75.0, 93.0, 131.0, 127.0, 130.0, 43.0, 37.0, 24.0, 23.0, 40.0, 51.0, 57.0, 54.0, 67.0, 198.0, 190.0, 210.0, 195.0, 275.0, 110.0, 100.0, 81.0, 92.0, 135.0, 155.0, 250.0, 200.0, 231.0, 212.0, 219.0, 227.0, 226.0, 225.0, 168.0, 169.0, 165.0, 167.0, 196.0, 119.0, 45.0, 47.0, 62.0, 35.0, 68.0, 88.0, 22.0, 17.0, 16.0, 18.0, 15.0, 13.0, 9.0, 11.0, 19.0, 12.0, 257.0, 174.0, 161.0, 137.0, 64.0, 87.0, 89.0, 94.0, 69.0, 102.0, 141.0, 211.0, 242.0, 271.0, 249.0, 147.0, 114.0, 108.0, 128.0, 150.0, 197.0, 235.0, 261.0, 269.0, 266.0, 263.0, 241.0, 205.0, 160.0, 208.0, 209.0, 183.0, 182.0, 230.0, 172.0, 166.0, 177.0, 191.0, 268.0, 317.0, 291.0, 313.0, 282.0, 303.0, 349.0, 407.0, 361.0, 234.0, 184.0, 146.0, 144.0, 173.0, 188.0, 203.0, 233.0, 408.0, 435.0, 403.0, 360.0, 402.0, 358.0, 383.0, 332.0, 274.0, 256.0, 302.0, 330.0, 367.0, 297.0, 331.0, 323.0, 339.0, 301.0, 310.0, 305.0, 336.0, 357.0, 485.0, 426.0, 374.0, 333.0, 343.0, 364.0, 389.0, 368.0, 267.0, 38.0, 85.0, 199.0, 340.0, 298.0, 299.0, 300.0, 240.0, 213.0, 171.0, 153.0, 143.0, 14.0, 10.0, 145.0, 101.0, 104.0, 97.0, 80.0, 121.0, 103.0, 99.0, 74.0, 189.0, 118.0, 125.0, 162.0, 157.0, 178.0, 187.0, 185.0, 180.0, 136.0, 133.0, 163.0, 258.0, 247.0, 273.0, 253.0, 254.0, 229.0, 193.0, 207.0, 176.0, 6.0, 8.0, 7.0, 46.0, 113.0, 259.0, 351.0, 980.0, 599.0, 116.0, 192.0, 237.0, 115.0, 265.0, 236.0, 296.0, 117.0, 112.0, 204.0, 122.0, 175.0, 215.0, 214.0, 206.0, 220.0, 111.0, 134.0, 142.0, 179.0, 218.0, 246.0, 264.0, 283.0, 223.0, 243.0, 262.0, 311.0, 307.0, 312.0, 292.0, 353.0, 309.0, 284.0, 244.0, 139.0, 248.0, 123.0, 202.0, 224.0, 252.0, 222.0, 255.0, 5.0, 216.0, 314.0, 201.0, 239.0, 228.0, 221.0, 217.0, 245.0, 232.0, 700.0, 473.0, 381.0, 315.0, 784.0, 761.0, 260.0, 280.0, 366.0, 288.0, 194.0, 186.0, 286.0, 289.0, 285.0, 151.0, 385.0, 382.0, 369.0, 342.0, 324.0, 272.0, 294.0, 4.0, 238.0, 251.0, 2.0, 3.0, 304.0, 290.0, 270.0, 278.0, 277.0, 276.0, 281.0, 306.0, 295.0, 318.0, 326.0, 337.0, 321.0, 411.0, 414.0, 334.0, 455.0, 440.0, 293.0, 1.0, 320.0, 372.0, 503.0, 534.0, 506.0, 505.0, 465.0, 468.0, 449.0, 418.0, 405.0, 388.0, 384.0, 359.0, 355.0, 348.0, 344.0, 279.0, 287.0, 327.0, 365.0, 391.0, 423.0, 429.0, 439.0, 458.0, 436.0, 431.0, 387.0, 350.0, 406.0, 422.0, 400.0, 470.0, 459.0, 378.0, 329.0, 335.0, 396.0, 412.0, 430.0, 434.0, 438.0, 416.0, 417.0, 394.0, 380.0, 376.0, 397.0, 392.0, 428.0, 420.0, 399.0, 393.0, 415.0, 395.0, 352.0, 419.0, 463.0, 481.0, 466.0, 479.0, 472.0, 467.0, 525.0, 504.0, 502.0, 521.0, 446.0, 354.0, 362.0, 325.0, 346.0, 476.0, 480.0, 482.0, 535.0, 569.0, 562.0, 488.0, 453.0, 501.0, 559.0, 557.0, 540.0, 371.0, 454.0, 328.0, 432.0, 433.0, 462.0, 398.0, 316.0, 379.0, 443.0, 322.0, 363.0, 319.0, 308.0, 528.0, 489.0, 404.0, 410.0, 441.0, 425.0, 427.0, 474.0, 500.0, 548.0, 584.0, 607.0, 611.0, 615.0, 587.0, 570.0, 573.0, 513.0, 345.0, 437.0, 477.0, 510.0, 526.0, 492.0, 527.0, 486.0, 493.0, 515.0, 595.0, 563.0, 495.0, 508.0, 464.0, 452.0, 498.0, 478.0, 499.0, 494.0, 445.0, 457.0, 347.0, 401.0, 373.0, 461.0, 421.0, 469.0, 409.0, 451.0, 424.0, 413.0, 377.0, 442.0, 375.0, 390.0, 356.0, 341.0, 338.0, 386.0, 514.0, 370.0, 460.0, 490.0, 522.0, 456.0, 524.0, 491.0, 484.0, 450.0, 558.0, 994.0, 972.0, 0.0, 529.0, 802.0, 845.0, 810.0, 776.0, 824.0, 886.0, 852.0, 858.0, 805.0, 744.0, 731.0, 722.0, 684.0, 651.0, 673.0, 475.0, 523.0, 447.0, 448.0, 603.0, 496.0, 444.0, 512.0, 532.0, 516.0, 541.0, 517.0, 539.0, 567.0, 659.0, 648.0, 671.0, 623.0, 618.0, 487.0, 579.0, 649.0, 507.0, 545.0, 577.0, 483.0, 519.0, 551.0, 542.0, 580.0]\n",
      ".........................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Importing Datasets\n",
    "aqi = pd.read_csv('PRSA_data.csv')\n",
    "aqi.head()\n",
    "aqi = aqi[['year','month','day','hour','pm2.5']]\n",
    "for i in aqi.columns:\n",
    "    print(i)\n",
    "    print(list(aqi[i].unique()))\n",
    "    print('.........................................................................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('month','day','hour'):\n",
    "    aqi[i] = aqi[i].astype(str)\n",
    "    aqi[i] = aqi[i].str.rjust(2,'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "[2010, 2011, 2012, 2013, 2014]\n",
      ".........................................................................................\n",
      "month\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
      ".........................................................................................\n",
      "day\n",
      "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
      ".........................................................................................\n",
      "hour\n",
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
      ".........................................................................................\n",
      "pm2.5\n",
      "[nan, 129.0, 148.0, 159.0, 181.0, 138.0, 109.0, 105.0, 124.0, 120.0, 132.0, 140.0, 152.0, 164.0, 158.0, 154.0, 170.0, 149.0, 156.0, 126.0, 90.0, 63.0, 65.0, 55.0, 83.0, 91.0, 86.0, 82.0, 78.0, 98.0, 107.0, 96.0, 95.0, 70.0, 61.0, 53.0, 71.0, 72.0, 76.0, 73.0, 79.0, 58.0, 25.0, 26.0, 28.0, 20.0, 29.0, 27.0, 32.0, 30.0, 31.0, 33.0, 34.0, 36.0, 39.0, 41.0, 50.0, 56.0, 59.0, 60.0, 84.0, 106.0, 66.0, 77.0, 44.0, 21.0, 42.0, 48.0, 49.0, 52.0, 75.0, 93.0, 131.0, 127.0, 130.0, 43.0, 37.0, 24.0, 23.0, 40.0, 51.0, 57.0, 54.0, 67.0, 198.0, 190.0, 210.0, 195.0, 275.0, 110.0, 100.0, 81.0, 92.0, 135.0, 155.0, 250.0, 200.0, 231.0, 212.0, 219.0, 227.0, 226.0, 225.0, 168.0, 169.0, 165.0, 167.0, 196.0, 119.0, 45.0, 47.0, 62.0, 35.0, 68.0, 88.0, 22.0, 17.0, 16.0, 18.0, 15.0, 13.0, 9.0, 11.0, 19.0, 12.0, 257.0, 174.0, 161.0, 137.0, 64.0, 87.0, 89.0, 94.0, 69.0, 102.0, 141.0, 211.0, 242.0, 271.0, 249.0, 147.0, 114.0, 108.0, 128.0, 150.0, 197.0, 235.0, 261.0, 269.0, 266.0, 263.0, 241.0, 205.0, 160.0, 208.0, 209.0, 183.0, 182.0, 230.0, 172.0, 166.0, 177.0, 191.0, 268.0, 317.0, 291.0, 313.0, 282.0, 303.0, 349.0, 407.0, 361.0, 234.0, 184.0, 146.0, 144.0, 173.0, 188.0, 203.0, 233.0, 408.0, 435.0, 403.0, 360.0, 402.0, 358.0, 383.0, 332.0, 274.0, 256.0, 302.0, 330.0, 367.0, 297.0, 331.0, 323.0, 339.0, 301.0, 310.0, 305.0, 336.0, 357.0, 485.0, 426.0, 374.0, 333.0, 343.0, 364.0, 389.0, 368.0, 267.0, 38.0, 85.0, 199.0, 340.0, 298.0, 299.0, 300.0, 240.0, 213.0, 171.0, 153.0, 143.0, 14.0, 10.0, 145.0, 101.0, 104.0, 97.0, 80.0, 121.0, 103.0, 99.0, 74.0, 189.0, 118.0, 125.0, 162.0, 157.0, 178.0, 187.0, 185.0, 180.0, 136.0, 133.0, 163.0, 258.0, 247.0, 273.0, 253.0, 254.0, 229.0, 193.0, 207.0, 176.0, 6.0, 8.0, 7.0, 46.0, 113.0, 259.0, 351.0, 980.0, 599.0, 116.0, 192.0, 237.0, 115.0, 265.0, 236.0, 296.0, 117.0, 112.0, 204.0, 122.0, 175.0, 215.0, 214.0, 206.0, 220.0, 111.0, 134.0, 142.0, 179.0, 218.0, 246.0, 264.0, 283.0, 223.0, 243.0, 262.0, 311.0, 307.0, 312.0, 292.0, 353.0, 309.0, 284.0, 244.0, 139.0, 248.0, 123.0, 202.0, 224.0, 252.0, 222.0, 255.0, 5.0, 216.0, 314.0, 201.0, 239.0, 228.0, 221.0, 217.0, 245.0, 232.0, 700.0, 473.0, 381.0, 315.0, 784.0, 761.0, 260.0, 280.0, 366.0, 288.0, 194.0, 186.0, 286.0, 289.0, 285.0, 151.0, 385.0, 382.0, 369.0, 342.0, 324.0, 272.0, 294.0, 4.0, 238.0, 251.0, 2.0, 3.0, 304.0, 290.0, 270.0, 278.0, 277.0, 276.0, 281.0, 306.0, 295.0, 318.0, 326.0, 337.0, 321.0, 411.0, 414.0, 334.0, 455.0, 440.0, 293.0, 1.0, 320.0, 372.0, 503.0, 534.0, 506.0, 505.0, 465.0, 468.0, 449.0, 418.0, 405.0, 388.0, 384.0, 359.0, 355.0, 348.0, 344.0, 279.0, 287.0, 327.0, 365.0, 391.0, 423.0, 429.0, 439.0, 458.0, 436.0, 431.0, 387.0, 350.0, 406.0, 422.0, 400.0, 470.0, 459.0, 378.0, 329.0, 335.0, 396.0, 412.0, 430.0, 434.0, 438.0, 416.0, 417.0, 394.0, 380.0, 376.0, 397.0, 392.0, 428.0, 420.0, 399.0, 393.0, 415.0, 395.0, 352.0, 419.0, 463.0, 481.0, 466.0, 479.0, 472.0, 467.0, 525.0, 504.0, 502.0, 521.0, 446.0, 354.0, 362.0, 325.0, 346.0, 476.0, 480.0, 482.0, 535.0, 569.0, 562.0, 488.0, 453.0, 501.0, 559.0, 557.0, 540.0, 371.0, 454.0, 328.0, 432.0, 433.0, 462.0, 398.0, 316.0, 379.0, 443.0, 322.0, 363.0, 319.0, 308.0, 528.0, 489.0, 404.0, 410.0, 441.0, 425.0, 427.0, 474.0, 500.0, 548.0, 584.0, 607.0, 611.0, 615.0, 587.0, 570.0, 573.0, 513.0, 345.0, 437.0, 477.0, 510.0, 526.0, 492.0, 527.0, 486.0, 493.0, 515.0, 595.0, 563.0, 495.0, 508.0, 464.0, 452.0, 498.0, 478.0, 499.0, 494.0, 445.0, 457.0, 347.0, 401.0, 373.0, 461.0, 421.0, 469.0, 409.0, 451.0, 424.0, 413.0, 377.0, 442.0, 375.0, 390.0, 356.0, 341.0, 338.0, 386.0, 514.0, 370.0, 460.0, 490.0, 522.0, 456.0, 524.0, 491.0, 484.0, 450.0, 558.0, 994.0, 972.0, 0.0, 529.0, 802.0, 845.0, 810.0, 776.0, 824.0, 886.0, 852.0, 858.0, 805.0, 744.0, 731.0, 722.0, 684.0, 651.0, 673.0, 475.0, 523.0, 447.0, 448.0, 603.0, 496.0, 444.0, 512.0, 532.0, 516.0, 541.0, 517.0, 539.0, 567.0, 659.0, 648.0, 671.0, 623.0, 618.0, 487.0, 579.0, 649.0, 507.0, 545.0, 577.0, 483.0, 519.0, 551.0, 542.0, 580.0]\n",
      ".........................................................................................\n"
     ]
    }
   ],
   "source": [
    "for i in aqi.columns:\n",
    "    print(i)\n",
    "    print(list(aqi[i].unique()))\n",
    "    print('.........................................................................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year month day hour  pm2.5        Date\n",
       "0  2010    01  01   00    NaN  01-01-2010\n",
       "1  2010    01  01   01    NaN  01-01-2010\n",
       "2  2010    01  01   02    NaN  01-01-2010\n",
       "3  2010    01  01   03    NaN  01-01-2010\n",
       "4  2010    01  01   04    NaN  01-01-2010"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqi['year'] = aqi['year'].astype(str)\n",
    "aqi['Date'] = aqi['day'] + '-' + aqi['month'] + '-' + aqi['year']\n",
    "aqi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>pm2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-2012</td>\n",
       "      <td>78.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      pm2.5\n",
       "0  01-01-2010        NaN\n",
       "1  01-01-2011        NaN\n",
       "2  01-01-2012  78.958333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqi['pm2.5'] = aqi['pm2.5'].astype(float)\n",
    "aqi_day = pd.DataFrame(aqi.groupby(['Date'])['pm2.5'].mean())\n",
    "aqi_day.reset_index(inplace=True)\n",
    "aqi_day.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df3 = climate_df2.copy()\n",
    "climate_df3.reset_index(inplace=True)\n",
    "climate_df3 = climate_df3.rename(columns = {'index':'Date'})\n",
    "# Merge Dataset with AQI\n",
    "climate_aqi = climate_df3.merge(aqi_day,on=['Date'],how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "            Date\n",
      "13-02-2011     1\n",
      "14-12-2012     1\n",
      "18-04-2012     1\n",
      "26-01-2014     1\n",
      "16-11-2014     1\n",
      "...          ...\n",
      "17-07-2010     1\n",
      "15-10-2011     1\n",
      "23-02-2012     1\n",
      "05-09-2013     1\n",
      "07-10-2012     1\n",
      "\n",
      "[1826 rows x 1 columns]\n",
      "......................................................\n",
      "T\n",
      "        T\n",
      "      891\n",
      "26.2   10\n",
      "-1.2    9\n",
      "26.3    9\n",
      "25.4    9\n",
      "...   ...\n",
      "-5.9    1\n",
      "8       1\n",
      "23.4    1\n",
      "29.9    1\n",
      "-5.6    1\n",
      "\n",
      "[349 rows x 1 columns]\n",
      "......................................................\n",
      "TM\n",
      "       TM\n",
      "      891\n",
      "32     31\n",
      "31     31\n",
      "28     25\n",
      "27     22\n",
      "...   ...\n",
      "25.8    1\n",
      "-2.9    1\n",
      "5.4     1\n",
      "32.9    1\n",
      "-1.2    1\n",
      "\n",
      "[299 rows x 1 columns]\n",
      "......................................................\n",
      "Tm\n",
      "       Tm\n",
      "      891\n",
      "21     30\n",
      "20     28\n",
      "-9     27\n",
      "-3     25\n",
      "...   ...\n",
      "21.3    1\n",
      "9.6     1\n",
      "-9.2    1\n",
      "5.4     1\n",
      "17.7    1\n",
      "\n",
      "[206 rows x 1 columns]\n",
      "......................................................\n",
      "H\n",
      "      H\n",
      "    891\n",
      "34   21\n",
      "29   21\n",
      "45   19\n",
      "72   19\n",
      "..  ...\n",
      "12    2\n",
      "97    1\n",
      "8     1\n",
      "95    1\n",
      "7     1\n",
      "\n",
      "[88 rows x 1 columns]\n",
      "......................................................\n",
      "PP\n",
      "        PP\n",
      "       891\n",
      "0      715\n",
      "-       36\n",
      "0.51    20\n",
      "1.02    17\n",
      "...    ...\n",
      "48.51    1\n",
      "17.27    1\n",
      "30.48    1\n",
      "85.09    1\n",
      "44.7     1\n",
      "\n",
      "[75 rows x 1 columns]\n",
      "......................................................\n",
      "VV\n",
      "       VV\n",
      "      891\n",
      "10.8  102\n",
      "-      71\n",
      "10     32\n",
      "10.5   25\n",
      "...   ...\n",
      "14      1\n",
      "11.1    1\n",
      "11.3    1\n",
      "13.2    1\n",
      "1.1     1\n",
      "\n",
      "[75 rows x 1 columns]\n",
      "......................................................\n",
      "V\n",
      "        V\n",
      "      891\n",
      "6.7    46\n",
      "7.4    43\n",
      "9.1    37\n",
      "8.3    31\n",
      "...   ...\n",
      "31.1    1\n",
      "18.5    1\n",
      "-       1\n",
      "30      1\n",
      "28.7    1\n",
      "\n",
      "[129 rows x 1 columns]\n",
      "......................................................\n",
      "VM\n",
      "       VM\n",
      "      891\n",
      "18    201\n",
      "14.4  187\n",
      "21.7  122\n",
      "25.2   88\n",
      "10.7   76\n",
      "32.4   59\n",
      "28.7   45\n",
      "35.9   39\n",
      "39.6   38\n",
      "43.2   23\n",
      "46.9   17\n",
      "50.4   12\n",
      "7.2    11\n",
      "53.9    8\n",
      "57.6    4\n",
      "61.1    3\n",
      "64.8    1\n",
      "-       1\n",
      "......................................................\n",
      "pm2.5\n",
      "            pm2.5\n",
      "39.416667       5\n",
      "65.000000       4\n",
      "31.916667       4\n",
      "23.500000       4\n",
      "22.000000       4\n",
      "...           ...\n",
      "43.916667       1\n",
      "200.291667      1\n",
      "134.583333      1\n",
      "56.125000       1\n",
      "69.000000       1\n",
      "\n",
      "[1534 rows x 1 columns]\n",
      "......................................................\n"
     ]
    }
   ],
   "source": [
    "for i in climate_aqi.columns:\n",
    "    print(i)\n",
    "    print(pd.DataFrame(climate_aqi[i].value_counts()))\n",
    "    print('......................................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_aqi = climate_aqi.replace(['-','',' '], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 10)\n",
      "(809, 10)\n"
     ]
    }
   ],
   "source": [
    "climate_aqi2 = climate_aqi.dropna()\n",
    "print(climate_aqi.shape)\n",
    "print(climate_aqi2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>H</th>\n",
       "      <th>PP</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>pm2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2010</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-9</td>\n",
       "      <td>83</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>145.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2010</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-13</td>\n",
       "      <td>76</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>39.6</td>\n",
       "      <td>78.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07-01-2010</td>\n",
       "      <td>-12.4</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-17</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>21.7</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08-01-2010</td>\n",
       "      <td>-10.9</td>\n",
       "      <td>-7</td>\n",
       "      <td>-16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>14.4</td>\n",
       "      <td>176.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09-01-2010</td>\n",
       "      <td>-8.7</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-15</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.2</td>\n",
       "      <td>88.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      T    TM   Tm   H   PP   VV     V    VM       pm2.5\n",
       "1  02-01-2010   -5.7  -3.8   -9  83  3.3  2.7  10.6    18  145.958333\n",
       "2  03-01-2010   -9.6  -6.4  -13  76  6.6    5  17.8  39.6   78.833333\n",
       "6  07-01-2010  -12.4  -4.4  -17  52    0    9   6.9  21.7   69.000000\n",
       "7  08-01-2010  -10.9    -7  -16  64    0  4.8   5.7  14.4  176.208333\n",
       "8  09-01-2010   -8.7  -2.8  -15  55    0  9.7  11.5  25.2   88.500000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_aqi2.head()\n",
    "# Convert all cols to numeric. Extract Month and Year. Convert year into a continuous varibale. Scale down. Regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>H</th>\n",
       "      <th>PP</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2010</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-9</td>\n",
       "      <td>83</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>145.958333</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2010</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-13</td>\n",
       "      <td>76</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>39.6</td>\n",
       "      <td>78.833333</td>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07-01-2010</td>\n",
       "      <td>-12.4</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-17</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>21.7</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>07</td>\n",
       "      <td>01</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08-01-2010</td>\n",
       "      <td>-10.9</td>\n",
       "      <td>-7</td>\n",
       "      <td>-16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>14.4</td>\n",
       "      <td>176.208333</td>\n",
       "      <td>08</td>\n",
       "      <td>01</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09-01-2010</td>\n",
       "      <td>-8.7</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-15</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25.2</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>09</td>\n",
       "      <td>01</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      T    TM   Tm   H   PP   VV     V    VM       pm2.5   0   1  \\\n",
       "1  02-01-2010   -5.7  -3.8   -9  83  3.3  2.7  10.6    18  145.958333  02  01   \n",
       "2  03-01-2010   -9.6  -6.4  -13  76  6.6    5  17.8  39.6   78.833333  03  01   \n",
       "6  07-01-2010  -12.4  -4.4  -17  52    0    9   6.9  21.7   69.000000  07  01   \n",
       "7  08-01-2010  -10.9    -7  -16  64    0  4.8   5.7  14.4  176.208333  08  01   \n",
       "8  09-01-2010   -8.7  -2.8  -15  55    0  9.7  11.5  25.2   88.500000  09  01   \n",
       "\n",
       "      2  \n",
       "1  2010  \n",
       "2  2010  \n",
       "6  2010  \n",
       "7  2010  \n",
       "8  2010  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clealry, pm2.5 concentration is our dependent variable while all other variables are independent variables. \n",
    "# Lets break up date to extract month and year. \n",
    "date_br = (climate_aqi2['Date'].str.split('-',expand=True))\n",
    "climate_aqi3 = pd.concat([climate_aqi2,date_br],axis=1)\n",
    "climate_aqi3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that::\n",
    "1. T: Average Temperature in C\n",
    "2. TM: Max Temp\n",
    "3. Tm: Min Temp\n",
    "4. H: Average Relative Humidity\n",
    "5. PP: Total Rainfall/Snow Melt\n",
    "6. VV: Visibility\n",
    "7. V: Average Wind Speed\n",
    "8. VM: Max Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>H</th>\n",
       "      <th>PP</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>pm2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987047</td>\n",
       "      <td>0.982146</td>\n",
       "      <td>0.335939</td>\n",
       "      <td>0.176769</td>\n",
       "      <td>-0.102042</td>\n",
       "      <td>-0.170312</td>\n",
       "      <td>-0.116175</td>\n",
       "      <td>-0.066941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TM</th>\n",
       "      <td>0.987047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949051</td>\n",
       "      <td>0.256045</td>\n",
       "      <td>0.152456</td>\n",
       "      <td>-0.053139</td>\n",
       "      <td>-0.157260</td>\n",
       "      <td>-0.085320</td>\n",
       "      <td>-0.075993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tm</th>\n",
       "      <td>0.982146</td>\n",
       "      <td>0.949051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456223</td>\n",
       "      <td>0.213545</td>\n",
       "      <td>-0.179405</td>\n",
       "      <td>-0.210083</td>\n",
       "      <td>-0.164535</td>\n",
       "      <td>-0.028740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.335939</td>\n",
       "      <td>0.256045</td>\n",
       "      <td>0.456223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309121</td>\n",
       "      <td>-0.645557</td>\n",
       "      <td>-0.558377</td>\n",
       "      <td>-0.491445</td>\n",
       "      <td>0.409420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PP</th>\n",
       "      <td>0.176769</td>\n",
       "      <td>0.152456</td>\n",
       "      <td>0.213545</td>\n",
       "      <td>0.309121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.121780</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.062112</td>\n",
       "      <td>0.008309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VV</th>\n",
       "      <td>-0.102042</td>\n",
       "      <td>-0.053139</td>\n",
       "      <td>-0.179405</td>\n",
       "      <td>-0.645557</td>\n",
       "      <td>-0.121780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446488</td>\n",
       "      <td>0.370986</td>\n",
       "      <td>-0.771454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>-0.170312</td>\n",
       "      <td>-0.157260</td>\n",
       "      <td>-0.210083</td>\n",
       "      <td>-0.558377</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.446488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833223</td>\n",
       "      <td>-0.346405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VM</th>\n",
       "      <td>-0.116175</td>\n",
       "      <td>-0.085320</td>\n",
       "      <td>-0.164535</td>\n",
       "      <td>-0.491445</td>\n",
       "      <td>0.062112</td>\n",
       "      <td>0.370986</td>\n",
       "      <td>0.833223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.270282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pm2.5</th>\n",
       "      <td>-0.066941</td>\n",
       "      <td>-0.075993</td>\n",
       "      <td>-0.028740</td>\n",
       "      <td>0.409420</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>-0.771454</td>\n",
       "      <td>-0.346405</td>\n",
       "      <td>-0.270282</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              T        TM        Tm         H        PP        VV         V  \\\n",
       "T      1.000000  0.987047  0.982146  0.335939  0.176769 -0.102042 -0.170312   \n",
       "TM     0.987047  1.000000  0.949051  0.256045  0.152456 -0.053139 -0.157260   \n",
       "Tm     0.982146  0.949051  1.000000  0.456223  0.213545 -0.179405 -0.210083   \n",
       "H      0.335939  0.256045  0.456223  1.000000  0.309121 -0.645557 -0.558377   \n",
       "PP     0.176769  0.152456  0.213545  0.309121  1.000000 -0.121780 -0.019579   \n",
       "VV    -0.102042 -0.053139 -0.179405 -0.645557 -0.121780  1.000000  0.446488   \n",
       "V     -0.170312 -0.157260 -0.210083 -0.558377 -0.019579  0.446488  1.000000   \n",
       "VM    -0.116175 -0.085320 -0.164535 -0.491445  0.062112  0.370986  0.833223   \n",
       "pm2.5 -0.066941 -0.075993 -0.028740  0.409420  0.008309 -0.771454 -0.346405   \n",
       "\n",
       "             VM     pm2.5  \n",
       "T     -0.116175 -0.066941  \n",
       "TM    -0.085320 -0.075993  \n",
       "Tm    -0.164535 -0.028740  \n",
       "H     -0.491445  0.409420  \n",
       "PP     0.062112  0.008309  \n",
       "VV     0.370986 -0.771454  \n",
       "V      0.833223 -0.346405  \n",
       "VM     1.000000 -0.270282  \n",
       "pm2.5 -0.270282  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(climate_aqi2.columns)\n",
    "x.extend(['Day', 'Month', 'Year'])\n",
    "climate_aqi3.columns = x\n",
    "climate_aqi3.head()\n",
    "num_cols = ['T','TM','Tm','H','PP','VV','V','VM','pm2.5']\n",
    "for i in num_cols:\n",
    "    climate_aqi3[i] = climate_aqi3[i].astype(float)\n",
    "\n",
    "climate_aqi3.corr()\n",
    "\n",
    "# We see that TM and Tm are highly corelated with each other and the column T. Hence, we drop these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have prepared our data well, let us now preprocess it\n",
    "# Drop Date, Day\n",
    "# All continuous variables except month. We get dummies for month. We will treat year as a continuous variable (why???)\n",
    "# Year will be treated as a continuous variable as AQI follows a general upward trend over a period.\n",
    "# Scale all variables, split data (Train test split) and then use the following models:\n",
    "# Linear Regression, Random Forest Regressor, Lasso Regression, ANN\n",
    "\n",
    "# Get Dummies for month column\n",
    "mon_dumm = pd.get_dummies(climate_aqi3['Month'],prefix='month_')\n",
    "climate_aqi4 = pd.concat([climate_aqi3,mon_dumm],axis=1)\n",
    "climate_aqi4.head()\n",
    "# Drop Unnecessary Columns and Create Dummies\n",
    "climate_aqi4.drop(['Date','Month','Day'],inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map year with labels\n",
    "yr_map = {'2010':1,'2011':2,'2012':3,'2013':4,'2014':5}\n",
    "climate_aqi4['Year'] = climate_aqi4['Year'].map(yr_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also define Dependant and Independent features\n",
    "climate_aqi4.columns\n",
    "X = ['T', 'H', 'PP', 'VV', 'V', 'VM','Year',\n",
    "       'month__01', 'month__02', 'month__03', 'month__04', 'month__05',\n",
    "       'month__06', 'month__07', 'month__08', 'month__09', 'month__10',\n",
    "       'month__11', 'month__12']\n",
    "Y = ['pm2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(647, 22)\n",
      "(162, 22)\n"
     ]
    }
   ],
   "source": [
    "# Convert all values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "climate_aqi_transformed = pd.DataFrame(scaler.fit_transform(climate_aqi4),columns = climate_aqi4.columns)\n",
    "\n",
    "# Get Train and Test data\n",
    "\n",
    "train,test = train_test_split(climate_aqi_transformed,test_size=0.2,random_state=1024)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all machine learning models \n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>pm2.5</td>      <th>  R-squared:         </th> <td>   0.685</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.676</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   76.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 07 Aug 2020</td> <th>  Prob (F-statistic):</th> <td>2.64e-144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:52:24</td>     <th>  Log-Likelihood:    </th> <td>  669.22</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   647</td>      <th>  AIC:               </th> <td>  -1300.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   628</td>      <th>  BIC:               </th> <td>  -1215.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    18</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    0.2785</td> <td>    0.041</td> <td>    6.856</td> <td> 0.000</td> <td>    0.199</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>T</th>         <td>    0.3136</td> <td>    0.051</td> <td>    6.151</td> <td> 0.000</td> <td>    0.214</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>H</th>         <td>    0.0262</td> <td>    0.027</td> <td>    0.961</td> <td> 0.337</td> <td>   -0.027</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PP</th>        <td>   -0.0479</td> <td>    0.058</td> <td>   -0.829</td> <td> 0.407</td> <td>   -0.161</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VV</th>        <td>   -0.5719</td> <td>    0.026</td> <td>  -21.931</td> <td> 0.000</td> <td>   -0.623</td> <td>   -0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V</th>         <td>   -0.0961</td> <td>    0.042</td> <td>   -2.301</td> <td> 0.022</td> <td>   -0.178</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VM</th>        <td>    0.0558</td> <td>    0.035</td> <td>    1.588</td> <td> 0.113</td> <td>   -0.013</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Year</th>      <td>   -0.0633</td> <td>    0.011</td> <td>   -5.918</td> <td> 0.000</td> <td>   -0.084</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__01</th> <td>    0.1738</td> <td>    0.021</td> <td>    8.408</td> <td> 0.000</td> <td>    0.133</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__02</th> <td>    0.1641</td> <td>    0.018</td> <td>    8.977</td> <td> 0.000</td> <td>    0.128</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__03</th> <td>    0.0955</td> <td>    0.013</td> <td>    7.449</td> <td> 0.000</td> <td>    0.070</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__04</th> <td>    0.0253</td> <td>    0.012</td> <td>    2.065</td> <td> 0.039</td> <td>    0.001</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__05</th> <td>   -0.0301</td> <td>    0.017</td> <td>   -1.783</td> <td> 0.075</td> <td>   -0.063</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__06</th> <td>   -0.0911</td> <td>    0.021</td> <td>   -4.412</td> <td> 0.000</td> <td>   -0.132</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__07</th> <td>   -0.1557</td> <td>    0.024</td> <td>   -6.494</td> <td> 0.000</td> <td>   -0.203</td> <td>   -0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__08</th> <td>   -0.1286</td> <td>    0.024</td> <td>   -5.422</td> <td> 0.000</td> <td>   -0.175</td> <td>   -0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__09</th> <td>   -0.0685</td> <td>    0.019</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.106</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__10</th> <td>    0.0357</td> <td>    0.012</td> <td>    2.901</td> <td> 0.004</td> <td>    0.012</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__11</th> <td>    0.1150</td> <td>    0.014</td> <td>    8.382</td> <td> 0.000</td> <td>    0.088</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month__12</th> <td>    0.1430</td> <td>    0.019</td> <td>    7.387</td> <td> 0.000</td> <td>    0.105</td> <td>    0.181</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>154.795</td> <th>  Durbin-Watson:     </th> <td>   1.984</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 548.630</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.090</td>  <th>  Prob(JB):          </th> <td>7.35e-120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.949</td>  <th>  Cond. No.          </th> <td>1.21e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 9.84e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  pm2.5   R-squared:                       0.685\n",
       "Model:                            OLS   Adj. R-squared:                  0.676\n",
       "Method:                 Least Squares   F-statistic:                     76.00\n",
       "Date:                Fri, 07 Aug 2020   Prob (F-statistic):          2.64e-144\n",
       "Time:                        02:52:24   Log-Likelihood:                 669.22\n",
       "No. Observations:                 647   AIC:                            -1300.\n",
       "Df Residuals:                     628   BIC:                            -1215.\n",
       "Df Model:                          18                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.2785      0.041      6.856      0.000       0.199       0.358\n",
       "T              0.3136      0.051      6.151      0.000       0.214       0.414\n",
       "H              0.0262      0.027      0.961      0.337      -0.027       0.080\n",
       "PP            -0.0479      0.058     -0.829      0.407      -0.161       0.065\n",
       "VV            -0.5719      0.026    -21.931      0.000      -0.623      -0.521\n",
       "V             -0.0961      0.042     -2.301      0.022      -0.178      -0.014\n",
       "VM             0.0558      0.035      1.588      0.113      -0.013       0.125\n",
       "Year          -0.0633      0.011     -5.918      0.000      -0.084      -0.042\n",
       "month__01      0.1738      0.021      8.408      0.000       0.133       0.214\n",
       "month__02      0.1641      0.018      8.977      0.000       0.128       0.200\n",
       "month__03      0.0955      0.013      7.449      0.000       0.070       0.121\n",
       "month__04      0.0253      0.012      2.065      0.039       0.001       0.049\n",
       "month__05     -0.0301      0.017     -1.783      0.075      -0.063       0.003\n",
       "month__06     -0.0911      0.021     -4.412      0.000      -0.132      -0.051\n",
       "month__07     -0.1557      0.024     -6.494      0.000      -0.203      -0.109\n",
       "month__08     -0.1286      0.024     -5.422      0.000      -0.175      -0.082\n",
       "month__09     -0.0685      0.019     -3.606      0.000      -0.106      -0.031\n",
       "month__10      0.0357      0.012      2.901      0.004       0.012       0.060\n",
       "month__11      0.1150      0.014      8.382      0.000       0.088       0.142\n",
       "month__12      0.1430      0.019      7.387      0.000       0.105       0.181\n",
       "==============================================================================\n",
       "Omnibus:                      154.795   Durbin-Watson:                   1.984\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              548.630\n",
       "Skew:                           1.090   Prob(JB):                    7.35e-120\n",
       "Kurtosis:                       6.949   Cond. No.                     1.21e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 9.84e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Linear Regression Model\n",
    "lin_reg = OLS(train[Y],add_constant(train[X])).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that H and PP have high p values\n",
    "# Clearly, we have columns that might need to be eliminated completly. Hence, in addition to Linear Regression, we will use Lasso regression\n",
    "# defining models and their parameters\n",
    "models = [LinearRegression(),Lasso(),DecisionTreeRegressor(random_state=1024,criterion='mse'),RandomForestRegressor(random_state=1024,criterion='mse')]\n",
    "grid = [{},{'alpha':[0.5,1,2,5,10]},{'min_samples_split':[2,10,20],'min_samples_leaf':[3,7]},\n",
    "        {'min_samples_split':[2,10,15],'min_samples_leaf':[2,4,5],'n_estimators':[10,50,100]}]\n",
    "result = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{}\n",
      "1\n",
      "{'alpha': 0.5}\n",
      "2\n",
      "{'min_samples_leaf': 7, 'min_samples_split': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use Grid Search CV for hyperparameter tuning\n",
    "# Use RMSE as cost function. We have used RMSE because we want to penalise large deviations more. A ballpark figure works.\n",
    "for i,j in enumerate(models):\n",
    "    clf = GridSearchCV(j, grid[i],scoring='neg_root_mean_squared_error',cv=5)\n",
    "    clf.fit(train[X],np.array(train[Y]))\n",
    "    df_temp = pd.DataFrame(clf.cv_results_)\n",
    "    if i==0:\n",
    "        result = df_temp.copy()\n",
    "    else:\n",
    "        result = result.append(df_temp,ignore_index=True)\n",
    "    print(i)\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.393767</td>\n",
       "      <td>0.020730</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>{'min_samples_leaf': 2, 'min_samples_split': 2...</td>\n",
       "      <td>-0.080582</td>\n",
       "      <td>-0.072837</td>\n",
       "      <td>-0.078370</td>\n",
       "      <td>-0.089950</td>\n",
       "      <td>-0.070645</td>\n",
       "      <td>-0.078477</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.335689</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>{'min_samples_leaf': 2, 'min_samples_split': 1...</td>\n",
       "      <td>-0.080701</td>\n",
       "      <td>-0.073275</td>\n",
       "      <td>-0.079203</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>-0.070576</td>\n",
       "      <td>-0.078838</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.328141</td>\n",
       "      <td>0.026146</td>\n",
       "      <td>0.021876</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>{'min_samples_leaf': 2, 'min_samples_split': 1...</td>\n",
       "      <td>-0.081447</td>\n",
       "      <td>-0.072942</td>\n",
       "      <td>-0.079329</td>\n",
       "      <td>-0.091223</td>\n",
       "      <td>-0.070269</td>\n",
       "      <td>-0.079042</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.371892</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>0.018751</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>{'min_samples_leaf': 4, 'min_samples_split': 2...</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>-0.074084</td>\n",
       "      <td>-0.078102</td>\n",
       "      <td>-0.090430</td>\n",
       "      <td>-0.071505</td>\n",
       "      <td>-0.079127</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.315641</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>{'min_samples_leaf': 4, 'min_samples_split': 1...</td>\n",
       "      <td>-0.081733</td>\n",
       "      <td>-0.074232</td>\n",
       "      <td>-0.078140</td>\n",
       "      <td>-0.090810</td>\n",
       "      <td>-0.070942</td>\n",
       "      <td>-0.079171</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14       0.393767      0.020730         0.012502        0.006251   \n",
       "17       0.335689      0.020165         0.011978        0.006074   \n",
       "20       0.328141      0.026146         0.021876        0.007655   \n",
       "23       0.371892      0.030299         0.018751        0.006251   \n",
       "26       0.315641      0.011693         0.015625        0.000001   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "14  {'min_samples_leaf': 2, 'min_samples_split': 2...          -0.080582   \n",
       "17  {'min_samples_leaf': 2, 'min_samples_split': 1...          -0.080701   \n",
       "20  {'min_samples_leaf': 2, 'min_samples_split': 1...          -0.081447   \n",
       "23  {'min_samples_leaf': 4, 'min_samples_split': 2...          -0.081514   \n",
       "26  {'min_samples_leaf': 4, 'min_samples_split': 1...          -0.081733   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "14          -0.072837          -0.078370          -0.089950   \n",
       "17          -0.073275          -0.079203          -0.090432   \n",
       "20          -0.072942          -0.079329          -0.091223   \n",
       "23          -0.074084          -0.078102          -0.090430   \n",
       "26          -0.074232          -0.078140          -0.090810   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "14          -0.070645        -0.078477        0.006771                1   \n",
       "17          -0.070576        -0.078838        0.006888                2   \n",
       "20          -0.070269        -0.079042        0.007327                3   \n",
       "23          -0.071505        -0.079127        0.006604                4   \n",
       "26          -0.070942        -0.079171        0.006859                5   \n",
       "\n",
       "   param_alpha param_min_samples_leaf param_min_samples_split  \\\n",
       "14         NaN                      2                       2   \n",
       "17         NaN                      2                      10   \n",
       "20         NaN                      2                      15   \n",
       "23         NaN                      4                       2   \n",
       "26         NaN                      4                      10   \n",
       "\n",
       "   param_n_estimators  \n",
       "14                100  \n",
       "17                100  \n",
       "20                100  \n",
       "23                100  \n",
       "26                100  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sort = result.sort_values('mean_test_score',ascending=False)\n",
    "result_sort.to_csv('HP Tuning Results ML.csv',index=False)\n",
    "result_sort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# We see that Random Forest Regressor has the best performance.\n",
    "# Lets build an ANN model\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(Dense(units = 15,kernel_initializer = 'he_normal',activation = 'relu',input_dim = 19 ))\n",
    "regressor.add(Dense(units = 10,kernel_initializer = 'he_normal',activation = 'relu'))\n",
    "regressor.add(Dense(units = 10,kernel_initializer = 'he_normal',activation = 'relu'))\n",
    "regressor.add(Dense(units = 6,kernel_initializer = 'he_normal',activation = 'relu'))\n",
    "regressor.add(Dense(units = 1,kernel_initializer = 'glorot_uniform',activation = 'sigmoid')) # used sigmoid because we need a bounded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/30\n",
      "517/517 [==============================] - 0s 544us/step - loss: 0.1744 - val_loss: 0.1411\n",
      "Epoch 2/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.1408 - val_loss: 0.1173\n",
      "Epoch 3/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.1207 - val_loss: 0.1018\n",
      "Epoch 4/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.1066 - val_loss: 0.0902\n",
      "Epoch 5/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0955 - val_loss: 0.0807\n",
      "Epoch 6/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0862 - val_loss: 0.0725\n",
      "Epoch 7/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0781 - val_loss: 0.0654\n",
      "Epoch 8/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0710 - val_loss: 0.0592\n",
      "Epoch 9/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0647 - val_loss: 0.0535\n",
      "Epoch 10/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0589 - val_loss: 0.0486\n",
      "Epoch 11/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0539 - val_loss: 0.0441\n",
      "Epoch 12/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0493 - val_loss: 0.0403\n",
      "Epoch 13/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0455 - val_loss: 0.0368\n",
      "Epoch 14/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0419 - val_loss: 0.0338\n",
      "Epoch 15/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0389 - val_loss: 0.0312\n",
      "Epoch 16/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0362 - val_loss: 0.0288\n",
      "Epoch 17/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0338 - val_loss: 0.0268\n",
      "Epoch 18/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0318 - val_loss: 0.0250\n",
      "Epoch 19/30\n",
      "517/517 [==============================] - 0s 60us/step - loss: 0.0300 - val_loss: 0.0236\n",
      "Epoch 20/30\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0285 - val_loss: 0.0223\n",
      "Epoch 21/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0272 - val_loss: 0.0212\n",
      "Epoch 22/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0261 - val_loss: 0.0203\n",
      "Epoch 23/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0251 - val_loss: 0.0195\n",
      "Epoch 24/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0243 - val_loss: 0.0189\n",
      "Epoch 25/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 26/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0231 - val_loss: 0.0179\n",
      "Epoch 27/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0226 - val_loss: 0.0175\n",
      "Epoch 28/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0221 - val_loss: 0.0171\n",
      "Epoch 29/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0218 - val_loss: 0.0168\n",
      "Epoch 30/30\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0214 - val_loss: 0.0165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x6820a7d1c8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "regressor.compile(optimizer='sgd',loss= 'mean_squared_error')\n",
    "regressor.fit(train[X],train[Y],epochs = 30,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "pred = regressor.predict(test[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions for MAE, MAPE and RMSE\n",
    "# mean absoute error\n",
    "def mae(actual,predict):\n",
    "    actual = np.array(actual)\n",
    "    predict = np.array(predict)\n",
    "    a = actual.shape\n",
    "    actual.shape = a[0]\n",
    "    b = predict.shape\n",
    "    predict.shape = b[0]\n",
    "    \n",
    "    mae = (sum(abs(actual-predict))/len(actual))\n",
    "    return mae\n",
    "\n",
    "def rmse(actual,predict):\n",
    "    actual = np.array(actual)\n",
    "    predict = np.array(predict)\n",
    "    a = actual.shape\n",
    "    actual.shape = a[0]\n",
    "    b = predict.shape\n",
    "    predict.shape = b[0]\n",
    "    \n",
    "    rms = np.sqrt(sum((actual-predict)**2)/len(actual))\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16353865697325623\n",
      "0.1204043756686256\n"
     ]
    }
   ],
   "source": [
    "print(rmse(test[Y],pred))\n",
    "print(mae(test[Y],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try different architectures for this model\n",
    "def model_builder_and_evaluator(n,neurons,dropout=0):\n",
    "    neurons = list(neurons)\n",
    "    if n == len(neurons):\n",
    "        if n==1:\n",
    "            regressor = Sequential()\n",
    "            regressor.add(Dense(units = neurons[0],kernel_initializer = 'he_normal',activation = 'relu',input_dim = 19 ))\n",
    "        else:\n",
    "            regressor = Sequential()\n",
    "            regressor.add(Dense(units = neurons[0],kernel_initializer = 'he_normal',activation = 'relu',input_dim = 19 ))\n",
    "            for i in (range(n-1)):\n",
    "                regressor.add(Dense(units = neurons[i+1],kernel_initializer = 'he_normal',activation = 'relu' ))\n",
    "                if i%2==0:\n",
    "                    regressor.add(Dropout(dropout))\n",
    "            \n",
    "        regressor.add(Dense(units = 1,kernel_initializer = 'glorot_uniform',activation = 'sigmoid'))        \n",
    "        regressor.compile(optimizer='sgd',loss= 'mean_squared_error')\n",
    "        regressor.fit(train[X],train[Y],epochs = 100,validation_split=0.2)\n",
    "        \n",
    "        pred = regressor.predict(test[X])\n",
    "        return rmse(test[Y],pred),mae(test[Y],pred)\n",
    "    else:\n",
    "        return 'ERROR: Number of layers should match with the list of count of neurons passed'\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = [3,5,7,9]\n",
    "neurons_ = [[200,200,100],[250,100,100,100,50],[200,100,100,100,100,50,50],[250,250,100,100,100,100,50,50,10]]\n",
    "dropout_ = [0,0.2,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 1ms/step - loss: 0.0713 - val_loss: 0.0432\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0421 - val_loss: 0.0289\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0324 - val_loss: 0.0234\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0211\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0264 - val_loss: 0.0198\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0252 - val_loss: 0.0191\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0245 - val_loss: 0.0187\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0240 - val_loss: 0.0183\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0236 - val_loss: 0.0180\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0232 - val_loss: 0.0178\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0229 - val_loss: 0.0175\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0226 - val_loss: 0.0173\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0223 - val_loss: 0.0170\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0219 - val_loss: 0.0168\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0217 - val_loss: 0.0166\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0214 - val_loss: 0.0164\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0211 - val_loss: 0.0162\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0209 - val_loss: 0.0161\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0207 - val_loss: 0.0159\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0204 - val_loss: 0.0157\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0202 - val_loss: 0.0156\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0200 - val_loss: 0.0154\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0197 - val_loss: 0.0152\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0195 - val_loss: 0.0151\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0193 - val_loss: 0.0149\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0191 - val_loss: 0.0148\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0189 - val_loss: 0.0146\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0187 - val_loss: 0.0145\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0185 - val_loss: 0.0143\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0183 - val_loss: 0.0142\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0182 - val_loss: 0.0140\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0180 - val_loss: 0.0139\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0178 - val_loss: 0.0138\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - ETA: 0s - loss: 0.017 - 0s 121us/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0175 - val_loss: 0.0135\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0173 - val_loss: 0.0134\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0172 - val_loss: 0.0133\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0170 - val_loss: 0.0132\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0169 - val_loss: 0.0131\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0167 - val_loss: 0.0130\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0166 - val_loss: 0.0129\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0164 - val_loss: 0.0127\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0163 - val_loss: 0.0126\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0162 - val_loss: 0.0125\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0160 - val_loss: 0.0124\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0159 - val_loss: 0.0123\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0158 - val_loss: 0.0123\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0157 - val_loss: 0.0122\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0155 - val_loss: 0.0121\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0154 - val_loss: 0.0120\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0153 - val_loss: 0.0119\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0152 - val_loss: 0.0118\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0151 - val_loss: 0.0118\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0150 - val_loss: 0.0117\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0149 - val_loss: 0.0116\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0148 - val_loss: 0.0116\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 272us/step - loss: 0.0147 - val_loss: 0.0115\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0146 - val_loss: 0.0114\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0145 - val_loss: 0.0113\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0144 - val_loss: 0.0113\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0143 - val_loss: 0.0112\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0142 - val_loss: 0.0111\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0141 - val_loss: 0.0111\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0140 - val_loss: 0.0110\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0139 - val_loss: 0.0110\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0139 - val_loss: 0.0109\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0138 - val_loss: 0.0108\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0137 - val_loss: 0.0108\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0136 - val_loss: 0.0107\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0135 - val_loss: 0.0107\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0134 - val_loss: 0.0106\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0134 - val_loss: 0.0106\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0133 - val_loss: 0.0105\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0132 - val_loss: 0.0105\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0132 - val_loss: 0.0104\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0131 - val_loss: 0.0104\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0130 - val_loss: 0.0103\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0129 - val_loss: 0.0103\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 121us/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0126 - val_loss: 0.0100\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0125 - val_loss: 0.0100\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0125 - val_loss: 0.0100\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0124 - val_loss: 0.0099\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0123 - val_loss: 0.0098\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0122 - val_loss: 0.0098\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0122 - val_loss: 0.0098\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 91us/step - loss: 0.0121 - val_loss: 0.0097\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0120 - val_loss: 0.0097\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0120 - val_loss: 0.0096\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 1ms/step - loss: 0.0531 - val_loss: 0.0316\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0393 - val_loss: 0.0246\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0313 - val_loss: 0.0226\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0293 - val_loss: 0.0219\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0272 - val_loss: 0.0214\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0285 - val_loss: 0.0211\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0268 - val_loss: 0.0208\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0269 - val_loss: 0.0206\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0264 - val_loss: 0.0204\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0264 - val_loss: 0.0201\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0198\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0241 - val_loss: 0.0195\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0259 - val_loss: 0.0193\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0248 - val_loss: 0.0189\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0250 - val_loss: 0.0186\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0239 - val_loss: 0.0183\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0235 - val_loss: 0.0181\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0236 - val_loss: 0.0179\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0249 - val_loss: 0.0178\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0225 - val_loss: 0.0176\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0238 - val_loss: 0.0174\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0221 - val_loss: 0.0172\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0213 - val_loss: 0.0170\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0225 - val_loss: 0.0169\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0219 - val_loss: 0.0168\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0205 - val_loss: 0.0164\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0213 - val_loss: 0.0162\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0208 - val_loss: 0.0160\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0200 - val_loss: 0.0159\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0158\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0201 - val_loss: 0.0156\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0204 - val_loss: 0.0155\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0203 - val_loss: 0.0154\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0202 - val_loss: 0.0152\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0194 - val_loss: 0.0151\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0149\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0200 - val_loss: 0.0148\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0188 - val_loss: 0.0146\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0195 - val_loss: 0.0144\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0174 - val_loss: 0.0142\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0191 - val_loss: 0.0140\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0139\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0172 - val_loss: 0.0138\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0183 - val_loss: 0.0138\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0178 - val_loss: 0.0136\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0175 - val_loss: 0.0134\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0176 - val_loss: 0.0135\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0186 - val_loss: 0.0134\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0181 - val_loss: 0.0133\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0179 - val_loss: 0.0131\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0174 - val_loss: 0.0131\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0177 - val_loss: 0.0131\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0162 - val_loss: 0.0129\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0180 - val_loss: 0.0128\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 121us/step - loss: 0.0176 - val_loss: 0.0128\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0171 - val_loss: 0.0127\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0170 - val_loss: 0.0126\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0168 - val_loss: 0.0125\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0164 - val_loss: 0.0124\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0169 - val_loss: 0.0124\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0167 - val_loss: 0.0124\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0123\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0155 - val_loss: 0.0122\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0165 - val_loss: 0.0121\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0165 - val_loss: 0.0119\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0160 - val_loss: 0.0119\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0168 - val_loss: 0.0119\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0150 - val_loss: 0.0118\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0157 - val_loss: 0.0117\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0151 - val_loss: 0.0117\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0166 - val_loss: 0.0116\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0159 - val_loss: 0.0117\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0161 - val_loss: 0.0116\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0153 - val_loss: 0.0115\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0151 - val_loss: 0.0115\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0161 - val_loss: 0.0116\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0150 - val_loss: 0.0114\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0150 - val_loss: 0.0113\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0144 - val_loss: 0.0113\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0154 - val_loss: 0.0112\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0112\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0148 - val_loss: 0.0111\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0112\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0146 - val_loss: 0.0112\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0150 - val_loss: 0.0112\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0151 - val_loss: 0.0111\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0151 - val_loss: 0.0110\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0143 - val_loss: 0.0109\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0148 - val_loss: 0.0108\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0144 - val_loss: 0.0108\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0148 - val_loss: 0.0108\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0143 - val_loss: 0.0107\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0145 - val_loss: 0.0107\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0145 - val_loss: 0.0107\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0149 - val_loss: 0.0107\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0149 - val_loss: 0.0106\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0139 - val_loss: 0.0106\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0140 - val_loss: 0.0106\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.1259 - val_loss: 0.0573\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0613 - val_loss: 0.0321\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0407 - val_loss: 0.0243\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0386 - val_loss: 0.0209\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0321 - val_loss: 0.0196\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0311 - val_loss: 0.0190\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0298 - val_loss: 0.0187\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0271 - val_loss: 0.0185\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0292 - val_loss: 0.0184\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0291 - val_loss: 0.0183\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0277 - val_loss: 0.0182\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0278 - val_loss: 0.0181\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0279 - val_loss: 0.0180\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0280 - val_loss: 0.0179\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0269 - val_loss: 0.0178\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0177\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0255 - val_loss: 0.0175\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0260 - val_loss: 0.0175\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0253 - val_loss: 0.0174\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0259 - val_loss: 0.0172\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0254 - val_loss: 0.0171\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0254 - val_loss: 0.0170\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0261 - val_loss: 0.0169\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0259 - val_loss: 0.0168\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0266 - val_loss: 0.0167\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0166\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0238 - val_loss: 0.0164\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0261 - val_loss: 0.0163\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0253 - val_loss: 0.0162\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0260 - val_loss: 0.0161\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0246 - val_loss: 0.0160\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0243 - val_loss: 0.0158\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0235 - val_loss: 0.0157\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0265 - val_loss: 0.0156\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 151us/step - loss: 0.0237 - val_loss: 0.0155\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0246 - val_loss: 0.0154\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0244 - val_loss: 0.0153\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0232 - val_loss: 0.0152\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0225 - val_loss: 0.0152\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0232 - val_loss: 0.0151\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0223 - val_loss: 0.0150\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0240 - val_loss: 0.0149\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0223 - val_loss: 0.0148\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0228 - val_loss: 0.0147\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0219 - val_loss: 0.0146\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0234 - val_loss: 0.0146\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0225 - val_loss: 0.0145\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0234 - val_loss: 0.0144\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0221 - val_loss: 0.0143\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0224 - val_loss: 0.0142\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0216 - val_loss: 0.0141\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0219 - val_loss: 0.0140\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0213 - val_loss: 0.0140\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0223 - val_loss: 0.0139\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0202 - val_loss: 0.0138\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0222 - val_loss: 0.0138\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0219 - val_loss: 0.0137\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0220 - val_loss: 0.0136\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0224 - val_loss: 0.0136\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0230 - val_loss: 0.0135\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0220 - val_loss: 0.0135\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0212 - val_loss: 0.0134\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0133\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0207 - val_loss: 0.0133\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0210 - val_loss: 0.0132\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0209 - val_loss: 0.0132\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0199 - val_loss: 0.0131\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0131\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0193 - val_loss: 0.0130\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0202 - val_loss: 0.0129\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0195 - val_loss: 0.0129\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0204 - val_loss: 0.0128\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0208 - val_loss: 0.0128\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0196 - val_loss: 0.0127\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0207 - val_loss: 0.0126\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0209 - val_loss: 0.0126\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0197 - val_loss: 0.0125\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0203 - val_loss: 0.0125\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0208 - val_loss: 0.0124\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0189 - val_loss: 0.0124\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0123\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0122\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0193 - val_loss: 0.0122\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0185 - val_loss: 0.0121\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0208 - val_loss: 0.0121\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0196 - val_loss: 0.0120\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0200 - val_loss: 0.0120\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0196 - val_loss: 0.0119\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0119\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0189 - val_loss: 0.0118\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0194 - val_loss: 0.0118\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0118\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0193 - val_loss: 0.0118\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0117\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0204 - val_loss: 0.0116\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0177 - val_loss: 0.0116\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0194 - val_loss: 0.0116\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0179 - val_loss: 0.0115\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0115\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0193 - val_loss: 0.0114\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.0626 - val_loss: 0.0319\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0309 - val_loss: 0.0198\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0232 - val_loss: 0.0170\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0212 - val_loss: 0.0160\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0204 - val_loss: 0.0156\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0199 - val_loss: 0.0153\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0195 - val_loss: 0.0151\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0149\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0189 - val_loss: 0.0147\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0186 - val_loss: 0.0145\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0183 - val_loss: 0.0143\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0181 - val_loss: 0.0141\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 121us/step - loss: 0.0178 - val_loss: 0.0139\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0173 - val_loss: 0.0136\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0171 - val_loss: 0.0134\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0169 - val_loss: 0.0132\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0166 - val_loss: 0.0131\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0164 - val_loss: 0.0130\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0162 - val_loss: 0.0128\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0160 - val_loss: 0.0127\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0158 - val_loss: 0.0126\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0157 - val_loss: 0.0124\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0155 - val_loss: 0.0123\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0152 - val_loss: 0.0122\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0149 - val_loss: 0.0119\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0147 - val_loss: 0.0118\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0145 - val_loss: 0.0117\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0144 - val_loss: 0.0115\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0142 - val_loss: 0.0114\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0140 - val_loss: 0.0113\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0139 - val_loss: 0.0112\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0136 - val_loss: 0.0110\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0134 - val_loss: 0.0109\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0133 - val_loss: 0.0108\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0131 - val_loss: 0.0107\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0130 - val_loss: 0.0106\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0128 - val_loss: 0.0105\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0127 - val_loss: 0.0104\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0125 - val_loss: 0.0104\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0124 - val_loss: 0.0103\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0122 - val_loss: 0.0101\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0115 - val_loss: 0.0097\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0112 - val_loss: 0.0095\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0110 - val_loss: 0.0094\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0106 - val_loss: 0.0091\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0103 - val_loss: 0.0090\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0102 - val_loss: 0.0090\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0099 - val_loss: 0.0089\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0099 - val_loss: 0.0088\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0098 - val_loss: 0.0087\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0097 - val_loss: 0.0087\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0095 - val_loss: 0.0086\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0094 - val_loss: 0.0085\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0093 - val_loss: 0.0085\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0089 - val_loss: 0.0083\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0088 - val_loss: 0.0083\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0088 - val_loss: 0.0083\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0087 - val_loss: 0.0082\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0086 - val_loss: 0.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0085 - val_loss: 0.0081\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0085 - val_loss: 0.0081\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.0629 - val_loss: 0.0375\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0403 - val_loss: 0.0253\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0319 - val_loss: 0.0203\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0286 - val_loss: 0.0182\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0268 - val_loss: 0.0174\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0169\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0250 - val_loss: 0.0166\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0263 - val_loss: 0.0164\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0263 - val_loss: 0.0162\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0221 - val_loss: 0.0161\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0224 - val_loss: 0.0160\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0240 - val_loss: 0.0158\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0219 - val_loss: 0.0155\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0230 - val_loss: 0.0154\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0258 - val_loss: 0.0153\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0214 - val_loss: 0.0151\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0216 - val_loss: 0.0150\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0226 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0211 - val_loss: 0.0145\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0225 - val_loss: 0.0143\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0213 - val_loss: 0.0141\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0215 - val_loss: 0.0140\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0221 - val_loss: 0.0138\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0220 - val_loss: 0.0137\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0218 - val_loss: 0.0135\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0200 - val_loss: 0.0133\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0216 - val_loss: 0.0131\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0211 - val_loss: 0.0130\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0196 - val_loss: 0.0129\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0213 - val_loss: 0.0128\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0196 - val_loss: 0.0127\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0205 - val_loss: 0.0126\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0125\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0196 - val_loss: 0.0123\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0201 - val_loss: 0.0122\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0188 - val_loss: 0.0122\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0190 - val_loss: 0.0120\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0200 - val_loss: 0.0119\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0201 - val_loss: 0.0119\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0185 - val_loss: 0.0117\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0183 - val_loss: 0.0117\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0200 - val_loss: 0.0115\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0185 - val_loss: 0.0115\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0114\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0167 - val_loss: 0.0113\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0191 - val_loss: 0.0112\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0185 - val_loss: 0.0110\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0110\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0177 - val_loss: 0.0109\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0181 - val_loss: 0.0108\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0174 - val_loss: 0.0107\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0180 - val_loss: 0.0107\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0194 - val_loss: 0.0106\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0184 - val_loss: 0.0106\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0172 - val_loss: 0.0105\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0187 - val_loss: 0.0105\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0181 - val_loss: 0.0104\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0166 - val_loss: 0.0102\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0181 - val_loss: 0.0101\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0174 - val_loss: 0.0100\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0184 - val_loss: 0.0100\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0159 - val_loss: 0.0100\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0157 - val_loss: 0.0098\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0169 - val_loss: 0.0097\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0165 - val_loss: 0.0096\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0175 - val_loss: 0.0096\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0171 - val_loss: 0.0096\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0161 - val_loss: 0.0095\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0165 - val_loss: 0.0095\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 151us/step - loss: 0.0178 - val_loss: 0.0094\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0167 - val_loss: 0.0093\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0162 - val_loss: 0.0092\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0164 - val_loss: 0.0091\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0161 - val_loss: 0.0091\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0163 - val_loss: 0.0091\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0147 - val_loss: 0.0090\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0164 - val_loss: 0.0090\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0166 - val_loss: 0.0089\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0159 - val_loss: 0.0088\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0179 - val_loss: 0.0089\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0149 - val_loss: 0.0088\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0158 - val_loss: 0.0087\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0087\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0152 - val_loss: 0.0086\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0086\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0152 - val_loss: 0.0085\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0151 - val_loss: 0.0085\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0141 - val_loss: 0.0084\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0153 - val_loss: 0.0084\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0153 - val_loss: 0.0083\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0149 - val_loss: 0.0083\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0155 - val_loss: 0.0083\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0151 - val_loss: 0.0082\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0149 - val_loss: 0.0082\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0158 - val_loss: 0.0082\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0149 - val_loss: 0.0081\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0141 - val_loss: 0.0081\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0144 - val_loss: 0.0081\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0142 - val_loss: 0.0082\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0150 - val_loss: 0.0081\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.1561 - val_loss: 0.0524\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0887 - val_loss: 0.0316\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0653 - val_loss: 0.0246\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0504 - val_loss: 0.0229\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0455 - val_loss: 0.0223\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0427 - val_loss: 0.0223\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0356 - val_loss: 0.0225\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0433 - val_loss: 0.0227\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0403 - val_loss: 0.0230\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0387 - val_loss: 0.0231\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0362 - val_loss: 0.0231\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0395 - val_loss: 0.0231\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0358 - val_loss: 0.0230\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0344 - val_loss: 0.0230\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0387 - val_loss: 0.0231\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0345 - val_loss: 0.0230\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0349 - val_loss: 0.0229\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0333 - val_loss: 0.0229\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0319 - val_loss: 0.0227\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0382 - val_loss: 0.0226\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0330 - val_loss: 0.0225\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0309 - val_loss: 0.0224\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0344 - val_loss: 0.0222\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0334 - val_loss: 0.0219\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0336 - val_loss: 0.0218\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0294 - val_loss: 0.0218\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0316 - val_loss: 0.0215\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0342 - val_loss: 0.0215\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0322 - val_loss: 0.0213\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0330 - val_loss: 0.0212\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0326 - val_loss: 0.0209\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0322 - val_loss: 0.0208\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0320 - val_loss: 0.0207\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0324 - val_loss: 0.0206\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0329 - val_loss: 0.0205\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0328 - val_loss: 0.0204\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0318 - val_loss: 0.0203\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0315 - val_loss: 0.0203\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0320 - val_loss: 0.0201\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0297 - val_loss: 0.0201\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0335 - val_loss: 0.0200\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0325 - val_loss: 0.0198\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0312 - val_loss: 0.0197\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0297 - val_loss: 0.0196\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0298 - val_loss: 0.0194\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0303 - val_loss: 0.0193\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0278 - val_loss: 0.0191\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 181us/step - loss: 0.0293 - val_loss: 0.0191\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0299 - val_loss: 0.0190\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0286 - val_loss: 0.0189\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0188\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0283 - val_loss: 0.0187\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0315 - val_loss: 0.0186\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0307 - val_loss: 0.0185\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0283 - val_loss: 0.0183\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0286 - val_loss: 0.0183\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0270 - val_loss: 0.0182\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0298 - val_loss: 0.0181\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0299 - val_loss: 0.0181\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0305 - val_loss: 0.0180\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0280 - val_loss: 0.0179\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0282 - val_loss: 0.0178\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0279 - val_loss: 0.0177\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0259 - val_loss: 0.0177\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0266 - val_loss: 0.0176\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0276 - val_loss: 0.0175\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0280 - val_loss: 0.0175\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0277 - val_loss: 0.0174\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0298 - val_loss: 0.0173\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0279 - val_loss: 0.0172\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0272 - val_loss: 0.0172\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0295 - val_loss: 0.0171\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0171\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0170\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0280 - val_loss: 0.0170\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0266 - val_loss: 0.0169\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0277 - val_loss: 0.0169\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0287 - val_loss: 0.0168\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0267 - val_loss: 0.0168\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0267 - val_loss: 0.0168\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0167\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0273 - val_loss: 0.0167\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0289 - val_loss: 0.0166\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0274 - val_loss: 0.0166\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0264 - val_loss: 0.0165\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0266 - val_loss: 0.0165\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0259 - val_loss: 0.0164\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0247 - val_loss: 0.0164\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0258 - val_loss: 0.0163\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0261 - val_loss: 0.0163\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0163\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0243 - val_loss: 0.0162\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0247 - val_loss: 0.0162\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0263 - val_loss: 0.0161\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0268 - val_loss: 0.0161\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0264 - val_loss: 0.0161\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0261 - val_loss: 0.0160\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0258 - val_loss: 0.0160\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0257 - val_loss: 0.0159\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.0846 - val_loss: 0.0349\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0319 - val_loss: 0.0228\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0260 - val_loss: 0.0206\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0245 - val_loss: 0.0199\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0238 - val_loss: 0.0193\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0232 - val_loss: 0.0188\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0226 - val_loss: 0.0184\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0221 - val_loss: 0.0179\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0216 - val_loss: 0.0176\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0212 - val_loss: 0.0172\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0208 - val_loss: 0.0170\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0204 - val_loss: 0.0167\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0200 - val_loss: 0.0164\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0197 - val_loss: 0.0161\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0192 - val_loss: 0.0159\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0189 - val_loss: 0.0156\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0185 - val_loss: 0.0154\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0182 - val_loss: 0.0152\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0179 - val_loss: 0.0150\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0177 - val_loss: 0.0148\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0174 - val_loss: 0.0146\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0172 - val_loss: 0.0144\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0169 - val_loss: 0.0143\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0167 - val_loss: 0.0142\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0165 - val_loss: 0.0140\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 151us/step - loss: 0.0162 - val_loss: 0.0138\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0160 - val_loss: 0.0136\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0158 - val_loss: 0.0135\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0153 - val_loss: 0.0132\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0150 - val_loss: 0.0130\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0148 - val_loss: 0.0128\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0146 - val_loss: 0.0127\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0145 - val_loss: 0.0126\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0143 - val_loss: 0.0125\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0141 - val_loss: 0.0124\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0140 - val_loss: 0.0123\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0138 - val_loss: 0.0122\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0137 - val_loss: 0.0120\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0135 - val_loss: 0.0119\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0134 - val_loss: 0.0119\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0132 - val_loss: 0.0118\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0131 - val_loss: 0.0117\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0130 - val_loss: 0.0116\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0128 - val_loss: 0.0116\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0125 - val_loss: 0.0113\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0123 - val_loss: 0.0112\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0122 - val_loss: 0.0111\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0120 - val_loss: 0.0110\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 121us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0098 - val_loss: 0.0094\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0097 - val_loss: 0.0093\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0097 - val_loss: 0.0093\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0093 - val_loss: 0.0090\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.0669 - val_loss: 0.0476\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0453 - val_loss: 0.0338\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0379 - val_loss: 0.0281\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 181us/step - loss: 0.0327 - val_loss: 0.0251\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0323 - val_loss: 0.0235\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0321 - val_loss: 0.0225\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0286 - val_loss: 0.0219\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0302 - val_loss: 0.0216\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0285 - val_loss: 0.0214\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0305 - val_loss: 0.0210\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0321 - val_loss: 0.0208\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0290 - val_loss: 0.0206\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0302 - val_loss: 0.0205\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0300 - val_loss: 0.0204\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0293 - val_loss: 0.0203\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0308 - val_loss: 0.0201\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0276 - val_loss: 0.0201\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0304 - val_loss: 0.0200\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0291 - val_loss: 0.0198\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0275 - val_loss: 0.0197\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0284 - val_loss: 0.0196\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0270 - val_loss: 0.0197\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0261 - val_loss: 0.0196\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0291 - val_loss: 0.0196\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0292 - val_loss: 0.0195\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0270 - val_loss: 0.0195\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0268 - val_loss: 0.0195\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0283 - val_loss: 0.0194\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0270 - val_loss: 0.0194\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0246 - val_loss: 0.0194\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0253 - val_loss: 0.0192\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0261 - val_loss: 0.0191\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0276 - val_loss: 0.0188\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0264 - val_loss: 0.0189\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0266 - val_loss: 0.0189\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0259 - val_loss: 0.0186\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0247 - val_loss: 0.0185\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0254 - val_loss: 0.0184\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0263 - val_loss: 0.0182\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0247 - val_loss: 0.0182\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0255 - val_loss: 0.0182\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0240 - val_loss: 0.0183\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0249 - val_loss: 0.0182\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0255 - val_loss: 0.0182\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0240 - val_loss: 0.0182\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0270 - val_loss: 0.0182\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0249 - val_loss: 0.0182\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0244 - val_loss: 0.0181\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0237 - val_loss: 0.0182\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0252 - val_loss: 0.0181\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0237 - val_loss: 0.0184\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0241 - val_loss: 0.0181\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0235 - val_loss: 0.0180\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0253 - val_loss: 0.0180\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0247 - val_loss: 0.0179\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0237 - val_loss: 0.0176\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0233 - val_loss: 0.0177\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0238 - val_loss: 0.0177\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0234 - val_loss: 0.0176\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0237 - val_loss: 0.0175\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0235 - val_loss: 0.0172\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0238 - val_loss: 0.0172\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0233 - val_loss: 0.0174\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0238 - val_loss: 0.0173\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0238 - val_loss: 0.0171\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0234 - val_loss: 0.0173\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0242 - val_loss: 0.0173\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0249 - val_loss: 0.0170\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0239 - val_loss: 0.0169\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0230 - val_loss: 0.0170\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0227 - val_loss: 0.0168\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0230 - val_loss: 0.0165\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0229 - val_loss: 0.0165\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0226 - val_loss: 0.0168\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0241 - val_loss: 0.0168\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0227 - val_loss: 0.0166\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0221 - val_loss: 0.0164\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0225 - val_loss: 0.0162\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0226 - val_loss: 0.0162\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0218 - val_loss: 0.0160\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0215 - val_loss: 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0225 - val_loss: 0.0161\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0216 - val_loss: 0.0160\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - ETA: 0s - loss: 0.020 - 0s 151us/step - loss: 0.0216 - val_loss: 0.0158\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0230 - val_loss: 0.0158\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0238 - val_loss: 0.0156\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0210 - val_loss: 0.0156\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0235 - val_loss: 0.0157\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0216 - val_loss: 0.0156\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0215 - val_loss: 0.0156\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0223 - val_loss: 0.0155\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0219 - val_loss: 0.0155\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0219 - val_loss: 0.0155\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0217 - val_loss: 0.0152\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0220 - val_loss: 0.0153\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0207 - val_loss: 0.0151\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0224 - val_loss: 0.0156\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0210 - val_loss: 0.0153\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0209 - val_loss: 0.0151\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.0422 - val_loss: 0.0352\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0399 - val_loss: 0.0339\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0365 - val_loss: 0.0326\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0393 - val_loss: 0.0316\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0397 - val_loss: 0.0313\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0370 - val_loss: 0.0303\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0364 - val_loss: 0.0302\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0336 - val_loss: 0.0306\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0360 - val_loss: 0.0303\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0337 - val_loss: 0.0304\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0331 - val_loss: 0.0301\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0349 - val_loss: 0.0299\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0349 - val_loss: 0.0298\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0347 - val_loss: 0.0299\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0342 - val_loss: 0.0300\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0350 - val_loss: 0.0300\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0333 - val_loss: 0.0302\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0353 - val_loss: 0.0299\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0341 - val_loss: 0.0299\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0345 - val_loss: 0.0294\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0365 - val_loss: 0.0293\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0327 - val_loss: 0.0293\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0306 - val_loss: 0.0294\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0326 - val_loss: 0.0293\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0331 - val_loss: 0.0294\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0306 - val_loss: 0.0301\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0337 - val_loss: 0.0301\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0334 - val_loss: 0.0302\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0332 - val_loss: 0.0308\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0304 - val_loss: 0.0314\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0323 - val_loss: 0.0315\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0357 - val_loss: 0.0320\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0321 - val_loss: 0.0319\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0322 - val_loss: 0.0319\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0314 - val_loss: 0.0320\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0316 - val_loss: 0.0322\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0329 - val_loss: 0.0324\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0298 - val_loss: 0.0326\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0313 - val_loss: 0.0326\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0305 - val_loss: 0.0328\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0301 - val_loss: 0.0330\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0280 - val_loss: 0.0327\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0320 - val_loss: 0.0330\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0308 - val_loss: 0.0332\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0318 - val_loss: 0.0333\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0307 - val_loss: 0.0337\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0289 - val_loss: 0.0338\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0309 - val_loss: 0.0337\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0290 - val_loss: 0.0338\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0279 - val_loss: 0.0340\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0286 - val_loss: 0.0341\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0312 - val_loss: 0.0341\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0289 - val_loss: 0.0341\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0288 - val_loss: 0.0343\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0292 - val_loss: 0.0342\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0295 - val_loss: 0.0342\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0278 - val_loss: 0.0344\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0292 - val_loss: 0.0343\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 181us/step - loss: 0.0293 - val_loss: 0.0343\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0280 - val_loss: 0.0348\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0281 - val_loss: 0.0346\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0302 - val_loss: 0.0342\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0294 - val_loss: 0.0344\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0298 - val_loss: 0.0343\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0304 - val_loss: 0.0347\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0275 - val_loss: 0.0344\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0300 - val_loss: 0.0344\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0307 - val_loss: 0.0341\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0300 - val_loss: 0.0339\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0277 - val_loss: 0.0336\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0299 - val_loss: 0.0338\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0276 - val_loss: 0.0336\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0268 - val_loss: 0.0334\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0265 - val_loss: 0.0335\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0275 - val_loss: 0.0334\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0280 - val_loss: 0.0335\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0276 - val_loss: 0.0334\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0291 - val_loss: 0.0333\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0282 - val_loss: 0.0335\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0279 - val_loss: 0.0334\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0270 - val_loss: 0.0334\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0274 - val_loss: 0.0335\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0276 - val_loss: 0.0335\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0294 - val_loss: 0.0334\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0269 - val_loss: 0.0335\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0262 - val_loss: 0.0337\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0286 - val_loss: 0.0335\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0261 - val_loss: 0.0335\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0259 - val_loss: 0.0332\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0269 - val_loss: 0.0333\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0259 - val_loss: 0.0330\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - ETA: 0s - loss: 0.028 - 0s 151us/step - loss: 0.0267 - val_loss: 0.0331\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0270 - val_loss: 0.0329\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0263 - val_loss: 0.0327\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0287 - val_loss: 0.0326\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0251 - val_loss: 0.0326\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0269 - val_loss: 0.0326\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 151us/step - loss: 0.0276 - val_loss: 0.0326\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.1218 - val_loss: 0.0374\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0301 - val_loss: 0.0194\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0224 - val_loss: 0.0181\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0213 - val_loss: 0.0176\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0207 - val_loss: 0.0172\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0203 - val_loss: 0.0166\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0197 - val_loss: 0.0162\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0193 - val_loss: 0.0159\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0189 - val_loss: 0.0156\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0186 - val_loss: 0.0152\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0183 - val_loss: 0.0150\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0179 - val_loss: 0.0147\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0176 - val_loss: 0.0145\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0174 - val_loss: 0.0143\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0171 - val_loss: 0.0140\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0168 - val_loss: 0.0136\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0165 - val_loss: 0.0135\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0162 - val_loss: 0.0133\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0158 - val_loss: 0.0129\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0155 - val_loss: 0.0128\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0153 - val_loss: 0.0126\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0151 - val_loss: 0.0124\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0149 - val_loss: 0.0123\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0147 - val_loss: 0.0121\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0145 - val_loss: 0.0119\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0144 - val_loss: 0.0118\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0142 - val_loss: 0.0116\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0140 - val_loss: 0.0115\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0138 - val_loss: 0.0114\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0132 - val_loss: 0.0109\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0130 - val_loss: 0.0108\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0129 - val_loss: 0.0106\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0127 - val_loss: 0.0105\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0125 - val_loss: 0.0104\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 212us/step - loss: 0.0122 - val_loss: 0.0103\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0121 - val_loss: 0.0101\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0119 - val_loss: 0.0101\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0115 - val_loss: 0.0098\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0114 - val_loss: 0.0097\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0105 - val_loss: 0.0092\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0103 - val_loss: 0.0091\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0102 - val_loss: 0.0091\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0100 - val_loss: 0.0090\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0099 - val_loss: 0.0089\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0098 - val_loss: 0.0090\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0098 - val_loss: 0.0089\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0097 - val_loss: 0.0090\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0096 - val_loss: 0.0087\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0095 - val_loss: 0.0087\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0094 - val_loss: 0.0087\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0094 - val_loss: 0.0087\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0093 - val_loss: 0.0087\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0092 - val_loss: 0.0086\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0091 - val_loss: 0.0085\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0089 - val_loss: 0.0085\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0089 - val_loss: 0.0085\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0088 - val_loss: 0.0084\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0084 - val_loss: 0.0088\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0078 - val_loss: 0.0083\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0076 - val_loss: 0.0078\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0076 - val_loss: 0.0078\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 181us/step - loss: 0.0074 - val_loss: 0.0078\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.1528 - val_loss: 0.0516\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0674 - val_loss: 0.0246\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0424 - val_loss: 0.0190\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0369 - val_loss: 0.0182\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0297 - val_loss: 0.0185\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0310 - val_loss: 0.0189\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0319 - val_loss: 0.0192\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0292 - val_loss: 0.0192\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0308 - val_loss: 0.0191\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0247 - val_loss: 0.0193\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0289 - val_loss: 0.0192\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0273 - val_loss: 0.0191\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0293 - val_loss: 0.0189\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0292 - val_loss: 0.0188\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0291 - val_loss: 0.0188\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0279 - val_loss: 0.0186\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 242us/step - loss: 0.0295 - val_loss: 0.0183\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0283 - val_loss: 0.0182\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0291 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0275 - val_loss: 0.0177\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0288 - val_loss: 0.0174\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0280 - val_loss: 0.0170\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0275 - val_loss: 0.0167\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0247 - val_loss: 0.0161\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0260 - val_loss: 0.0157\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0258 - val_loss: 0.0156\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0271 - val_loss: 0.0153\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0257 - val_loss: 0.0152\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0248 - val_loss: 0.0149\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0270 - val_loss: 0.0148\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0248 - val_loss: 0.0149\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0252 - val_loss: 0.0146\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0277 - val_loss: 0.0147\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0245 - val_loss: 0.0146\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0247 - val_loss: 0.0144\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0243 - val_loss: 0.0141\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0245 - val_loss: 0.0142\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0230 - val_loss: 0.0139\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0252 - val_loss: 0.0138\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0240 - val_loss: 0.0138\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0246 - val_loss: 0.0138\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0238 - val_loss: 0.0137\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0232 - val_loss: 0.0136\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0237 - val_loss: 0.0134\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0233 - val_loss: 0.0133\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0231 - val_loss: 0.0132\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0227 - val_loss: 0.0131\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0223 - val_loss: 0.0131\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0230 - val_loss: 0.0129\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0244 - val_loss: 0.0129\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0237 - val_loss: 0.0128\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0236 - val_loss: 0.0128\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0224 - val_loss: 0.0127\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0212 - val_loss: 0.0127\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0221 - val_loss: 0.0126\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0220 - val_loss: 0.0125\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0229 - val_loss: 0.0124\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0216 - val_loss: 0.0123\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0220 - val_loss: 0.0121\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0214 - val_loss: 0.0120\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0220 - val_loss: 0.0120\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0226 - val_loss: 0.0118\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0212 - val_loss: 0.0117\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0219 - val_loss: 0.0118\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0208 - val_loss: 0.0116\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0214 - val_loss: 0.0115\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0214 - val_loss: 0.0115\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0210 - val_loss: 0.0116\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0210 - val_loss: 0.0115\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0200 - val_loss: 0.0114\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0203 - val_loss: 0.0113\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0218 - val_loss: 0.0113\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0206 - val_loss: 0.0113\n",
      "Epoch 74/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0203 - val_loss: 0.0112\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0213 - val_loss: 0.0111\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0212 - val_loss: 0.0111\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0198 - val_loss: 0.0110\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0220 - val_loss: 0.0109\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0205 - val_loss: 0.0108\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0191 - val_loss: 0.0108\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0207 - val_loss: 0.0107\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0194 - val_loss: 0.0107\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0213 - val_loss: 0.0107\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0201 - val_loss: 0.0106\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0205 - val_loss: 0.0106\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0195 - val_loss: 0.0106\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0198 - val_loss: 0.0105\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0202 - val_loss: 0.0104\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0183 - val_loss: 0.0104\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0188 - val_loss: 0.0104\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0198 - val_loss: 0.0103\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0186 - val_loss: 0.0103\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0184 - val_loss: 0.0103\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0196 - val_loss: 0.0102\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0182 - val_loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0190 - val_loss: 0.0101\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0207 - val_loss: 0.0101\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0209 - val_loss: 0.0101\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0189 - val_loss: 0.0101\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0196 - val_loss: 0.0101\n",
      "Train on 517 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "517/517 [==============================] - 1s 2ms/step - loss: 0.2223 - val_loss: 0.0769\n",
      "Epoch 2/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.1278 - val_loss: 0.0454\n",
      "Epoch 3/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0838 - val_loss: 0.0329\n",
      "Epoch 4/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0757 - val_loss: 0.0277\n",
      "Epoch 5/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0616 - val_loss: 0.0248\n",
      "Epoch 6/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0637 - val_loss: 0.0234\n",
      "Epoch 7/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0594 - val_loss: 0.0225\n",
      "Epoch 8/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0536 - val_loss: 0.0216\n",
      "Epoch 9/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0483 - val_loss: 0.0211\n",
      "Epoch 10/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0471 - val_loss: 0.0209\n",
      "Epoch 11/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0507 - val_loss: 0.0206\n",
      "Epoch 12/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0482 - val_loss: 0.0204\n",
      "Epoch 13/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0483 - val_loss: 0.0203\n",
      "Epoch 14/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0463 - val_loss: 0.0203\n",
      "Epoch 15/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0449 - val_loss: 0.0204\n",
      "Epoch 16/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0459 - val_loss: 0.0206\n",
      "Epoch 17/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0475 - val_loss: 0.0205\n",
      "Epoch 18/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0444 - val_loss: 0.0205\n",
      "Epoch 19/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0420 - val_loss: 0.0206\n",
      "Epoch 20/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0394 - val_loss: 0.0207\n",
      "Epoch 21/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0465 - val_loss: 0.0206\n",
      "Epoch 22/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0418 - val_loss: 0.0211\n",
      "Epoch 23/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0411 - val_loss: 0.0212\n",
      "Epoch 24/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0460 - val_loss: 0.0214\n",
      "Epoch 25/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0445 - val_loss: 0.0216\n",
      "Epoch 26/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0446 - val_loss: 0.0218\n",
      "Epoch 27/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0442 - val_loss: 0.0219\n",
      "Epoch 28/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0443 - val_loss: 0.0224\n",
      "Epoch 29/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0397 - val_loss: 0.0224\n",
      "Epoch 30/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0401 - val_loss: 0.0225\n",
      "Epoch 31/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0408 - val_loss: 0.0226\n",
      "Epoch 32/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0415 - val_loss: 0.0231\n",
      "Epoch 33/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0389 - val_loss: 0.0232\n",
      "Epoch 34/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0419 - val_loss: 0.0235\n",
      "Epoch 35/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0413 - val_loss: 0.0243\n",
      "Epoch 36/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0402 - val_loss: 0.0249\n",
      "Epoch 37/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0393 - val_loss: 0.0255\n",
      "Epoch 38/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0397 - val_loss: 0.0258\n",
      "Epoch 39/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0413 - val_loss: 0.0258\n",
      "Epoch 40/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0373 - val_loss: 0.0268\n",
      "Epoch 41/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0375 - val_loss: 0.0273\n",
      "Epoch 42/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0388 - val_loss: 0.0274\n",
      "Epoch 43/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0375 - val_loss: 0.0275\n",
      "Epoch 44/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0350 - val_loss: 0.0275\n",
      "Epoch 45/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0380 - val_loss: 0.0278\n",
      "Epoch 46/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0344 - val_loss: 0.0283\n",
      "Epoch 47/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0350 - val_loss: 0.0280\n",
      "Epoch 48/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0376 - val_loss: 0.0279\n",
      "Epoch 49/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0399 - val_loss: 0.0283\n",
      "Epoch 50/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0350 - val_loss: 0.0287\n",
      "Epoch 51/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0371 - val_loss: 0.0291\n",
      "Epoch 52/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0348 - val_loss: 0.0294\n",
      "Epoch 53/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0349 - val_loss: 0.0294\n",
      "Epoch 54/100\n",
      "517/517 [==============================] - ETA: 0s - loss: 0.038 - 0s 212us/step - loss: 0.0350 - val_loss: 0.0295\n",
      "Epoch 55/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0394 - val_loss: 0.0295\n",
      "Epoch 56/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0352 - val_loss: 0.0300\n",
      "Epoch 57/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0364 - val_loss: 0.0304\n",
      "Epoch 58/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0341 - val_loss: 0.0303\n",
      "Epoch 59/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0339 - val_loss: 0.0301\n",
      "Epoch 60/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0369 - val_loss: 0.0300\n",
      "Epoch 61/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0357 - val_loss: 0.0309\n",
      "Epoch 62/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0336 - val_loss: 0.0311\n",
      "Epoch 63/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0347 - val_loss: 0.0312\n",
      "Epoch 64/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0372 - val_loss: 0.0316\n",
      "Epoch 65/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0374 - val_loss: 0.0315\n",
      "Epoch 66/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0376 - val_loss: 0.0316\n",
      "Epoch 67/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0337 - val_loss: 0.0322\n",
      "Epoch 68/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0341 - val_loss: 0.0324\n",
      "Epoch 69/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0377 - val_loss: 0.0320\n",
      "Epoch 70/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0358 - val_loss: 0.0327\n",
      "Epoch 71/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0313 - val_loss: 0.0326\n",
      "Epoch 72/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0358 - val_loss: 0.0321\n",
      "Epoch 73/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0348 - val_loss: 0.0321\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 0s 242us/step - loss: 0.0343 - val_loss: 0.0325\n",
      "Epoch 75/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0341 - val_loss: 0.0327\n",
      "Epoch 76/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0331 - val_loss: 0.0324\n",
      "Epoch 77/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0323 - val_loss: 0.0323\n",
      "Epoch 78/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0368 - val_loss: 0.0322\n",
      "Epoch 79/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0347 - val_loss: 0.0324\n",
      "Epoch 80/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0349 - val_loss: 0.0323\n",
      "Epoch 81/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0357 - val_loss: 0.0327\n",
      "Epoch 82/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0324 - val_loss: 0.0329\n",
      "Epoch 83/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0335 - val_loss: 0.0330\n",
      "Epoch 84/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0348 - val_loss: 0.0334\n",
      "Epoch 85/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0329 - val_loss: 0.0335\n",
      "Epoch 86/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0328 - val_loss: 0.0334\n",
      "Epoch 87/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0327 - val_loss: 0.0338\n",
      "Epoch 88/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0311 - val_loss: 0.0340\n",
      "Epoch 89/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0319 - val_loss: 0.0341\n",
      "Epoch 90/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0332 - val_loss: 0.0344\n",
      "Epoch 91/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0320 - val_loss: 0.0345\n",
      "Epoch 92/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0318 - val_loss: 0.0344\n",
      "Epoch 93/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0315 - val_loss: 0.0345\n",
      "Epoch 94/100\n",
      "517/517 [==============================] - 0s 212us/step - loss: 0.0319 - val_loss: 0.0354\n",
      "Epoch 95/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0338 - val_loss: 0.0352\n",
      "Epoch 96/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0354 - val_loss: 0.0352\n",
      "Epoch 97/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0314 - val_loss: 0.0352\n",
      "Epoch 98/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0342 - val_loss: 0.0350\n",
      "Epoch 99/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0316 - val_loss: 0.0352\n",
      "Epoch 100/100\n",
      "517/517 [==============================] - 0s 242us/step - loss: 0.0348 - val_loss: 0.0351\n"
     ]
    }
   ],
   "source": [
    "result= []\n",
    "\n",
    "for n,neurons in zip(n_,neurons_):\n",
    "    for dropout in dropout_:\n",
    "        rms,ma = model_builder_and_evaluator(n,neurons,dropout)\n",
    "        strin = 'Layers:  ' + str(n) + '\\nDropout:  '+ str(dropout) +'\\n RMSE:  ' + str(rms) +'\\n MAE:  ' + str(ma) \n",
    "        result.append(strin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Layers:  3\\nDropout:  0\\n RMSE:  0.13023125094896906\\n MAE:  0.09426075795174196',\n",
       " 'Layers:  3\\nDropout:  0.2\\n RMSE:  0.13546242172230563\\n MAE:  0.09346731696002811',\n",
       " 'Layers:  3\\nDropout:  0.5\\n RMSE:  0.14880084315855555\\n MAE:  0.10533005673420383',\n",
       " 'Layers:  5\\nDropout:  0\\n RMSE:  0.12102322492923284\\n MAE:  0.08440197301795563',\n",
       " 'Layers:  5\\nDropout:  0.2\\n RMSE:  0.12279424331745166\\n MAE:  0.0855975351878028',\n",
       " 'Layers:  5\\nDropout:  0.5\\n RMSE:  0.16119852381230274\\n MAE:  0.11480108834453603',\n",
       " 'Layers:  7\\nDropout:  0\\n RMSE:  0.12045417036246088\\n MAE:  0.08747075106296016',\n",
       " 'Layers:  7\\nDropout:  0.2\\n RMSE:  0.1532530104123326\\n MAE:  0.12023993507815002',\n",
       " 'Layers:  7\\nDropout:  0.5\\n RMSE:  0.20170859207206687\\n MAE:  0.1752648232649218',\n",
       " 'Layers:  9\\nDropout:  0\\n RMSE:  0.10711302444112807\\n MAE:  0.07772608337282066',\n",
       " 'Layers:  9\\nDropout:  0.2\\n RMSE:  0.13886680469570375\\n MAE:  0.09482135966368223',\n",
       " 'Layers:  9\\nDropout:  0.5\\n RMSE:  0.1999492853264609\\n MAE:  0.1721837937890213']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Kapoor\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08546553029185722"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the best performance (RMSE = 0.10) with layers = 9 and dropout =0 Clearly, our model is not overfiting, so it is okay to have dropout =0\n",
    "# Lets revisit our RandomForest model with n_estimators =100 and min_sample_leaf = 2\n",
    "aqi_prediction_model = RandomForestRegressor(n_estimators=100,min_samples_leaf = 2, random_state=1024,criterion='mse')\n",
    "reg.fit(train[X],train[Y])\n",
    "pred = reg.predict(test[X])\n",
    "rmse(test[Y],pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('aqi_prediction_model.pickle','wb')\n",
    "pickle.dump(aqi_prediction_model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see that Random Forest model performs much better than the ANN model. \n",
    "### Hence, we'll go ahead with the Random Forest Regressor with min_sample_leaf as 2 and 100 trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
